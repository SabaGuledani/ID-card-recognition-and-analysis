{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0530cf7-be8f-417d-ae19-2f0b7c3c23ea",
   "metadata": {},
   "source": [
    "# Start\n",
    "these libraries are neccesary to install to use this code. specific versions must be installed. just run the code below for the first time when you will run this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a23dbb-93be-44e5-9d2d-b37db1ec3108",
   "metadata": {},
   "source": [
    "## Important variables to set before running code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecef956-02f5-476a-90ba-eafdfd40aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set those thresholds based on your needs\n",
    "blurryness_threshold = 50\n",
    "reflection_threshold = 0.7\n",
    "yolo_confidence_score_threshold = 0.8\n",
    "confidence_score_classification_threshold = 0.9\n",
    "classification_threshold = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5794efd-38da-4e94-aa8a-87ad915d18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pillow==10.4.0\n",
    "# !pip install tensorflow==2.8.3\n",
    "# !pip install ultralytics==8.3.23\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install opencv-python==4.10.0.84\n",
    "# !pip install numpy==1.26.4\n",
    "# !pip install pytesseract==0.3.13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c722543-a20d-4fda-9931-9461f2e6892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Input,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import RandomCrop, RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras import backend as K\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from tkinter import simpledialog, filedialog\n",
    "import pytesseract \n",
    "from deepface import DeepFace\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Clear last session to feee up space before new\n",
    "K.clear_session()\n",
    "\n",
    "# if GPU is available this code will state 1\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# image height and width for input into Siamese Network with MobileNetV3 as a base \n",
    "IMG_HEIGHT = 300\n",
    "IMG_WIDTH = 300\n",
    "# important for tesseract to work in jupyter notebook\n",
    "# PATH is relative chooose the one where tesseract.exe is installed\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe' \n",
    "\n",
    "\n",
    "# Options for the dropdown menu\n",
    "languages = {\"Georgian\":\"kat\", \"French\": \"fra\", \"German\": \"deu\", \"Spanish\" :\"spa\", \"Russian\":\"rus\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5fa42f-07fc-44e2-a40f-35f4c56092c9",
   "metadata": {},
   "source": [
    "# AI code part for object detection and id card recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87744d82-8f01-420c-82a8-0513700c7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Siamese network model file\n",
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model\n",
    "\n",
    "# Preprocessing function to resize and normalize image\n",
    "def preprocess_image(image):\n",
    "    # resize image to match height and width specified earlier \n",
    "    image_resized = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    \n",
    "    # Normalize values of pixels to be around 0.0-1.0\n",
    "    image_resized = image_resized / 255.0  \n",
    "    \n",
    "    return image_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2cadd-d7b3-443e-9aba-5fc5a79e95ec",
   "metadata": {},
   "source": [
    "## Id cad type prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bd57ed-f88e-448b-bfce-8e14b644266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese network prediction function  \n",
    "def predict(id_images, img_to_evaluate, model, threshold = classification_threshold):\n",
    "    '''\n",
    "    id_images - batch of saved id images, should be numpy array of shape (x, 300, 300, 3) where x is number of saved images \n",
    "    or path for image which is saved in the memory and should be compared to, in this case it is string path\n",
    "    img_to_evaluate - image which will be compared to img_base which is saved in the memory\n",
    "    model - AI model file which contains trained weights for siamese network\n",
    "    threshold - optional value which we can state for siamese networks confidence score from 0.50 till 0.99, which means \n",
    "    that results will be shown only if confidence score for similarity is more than threshold\n",
    "    '''\n",
    "    \n",
    "    # preprocess images\n",
    "    if type(id_images) == str:\n",
    "        img_base = np.array(preprocess_image(id_images))\n",
    "        img_base = np.expand_dims(img_base, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    elif isinstance(id_images, np.ndarray):\n",
    "        img_base = id_images\n",
    "\n",
    "    \n",
    "    img_to_evaluate = np.array(preprocess_image(img_to_evaluate))\n",
    "\n",
    "    # expand dimensions of images to add batch dimension \n",
    "    \n",
    "    img_to_evaluate = np.expand_dims(img_to_evaluate, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    \n",
    "    img_to_evaluate = np.tile(img_to_evaluate, (len(img_base), 1, 1, 1))\n",
    "    \n",
    "    # predict the similarity between the two images,\n",
    "    # returns tensor with two values like this [0.37, 0.63] which states how similar are images\n",
    "    # 0.0 no similarity , 1.0 full similarity\n",
    "    preds = model.predict((img_base,img_to_evaluate))\n",
    "    predicted_idx = np.argmax(preds[:, 1])\n",
    "\n",
    "    if preds[predicted_idx][1] > threshold:\n",
    "        return predicted_idx, preds[predicted_idx][1]\n",
    "    else:\n",
    "        return 999,999\n",
    "    # if the model is sure that shown images are more similar than threshold it will output the 1, if not 0\n",
    "    # if predictions are less than a threshold it will output 2   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78faf1-1e67-4345-b0e4-1b702dee5834",
   "metadata": {},
   "source": [
    "### Load models for Object detection and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aaa0955-860a-436d-921f-eb6d80b6f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Saba/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-1-5 Python-3.9.21 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# load the siamese network model\n",
    "model_name = f'MobileNetV3_1024_siamese.h5'\n",
    "model = load_model(model_name)\n",
    "# load the YOLO model \n",
    "yolo_model = torch.hub.load('ultralytics/yolov5',\"custom\", path=r'.\\yolov5\\runs\\train\\exp6\\weights/best.pt',force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e51b31-05a5-4ce7-9540-a548550f6a65",
   "metadata": {},
   "source": [
    "## YOLO object detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cddec8c6-3b4a-4944-9808-293ab073cfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "# function returns YOLO model object detection results \n",
    "def yolo(yolo_model,frame):\n",
    "    results = yolo_model(frame)\n",
    "    return results\n",
    "\n",
    "kk = cv2.imread(\"./id_examples/driving_license_geo_20241219_102226.jpg\")\n",
    "kk = cv2.resize(kk,(300,300))\n",
    "print(kk.shape)\n",
    "kkk = yolo(yolo_model,kk)\n",
    "predict(np.array([kk]),kk,model)\n",
    "# unpack the coordinates of bounding boxes and confidence score for ID class\n",
    "def get_results_of_yolo(frame_predictions):\n",
    "    x1 = int(frame_predictions.xyxy[0][0][0])\n",
    "    y1 = int(frame_predictions.xyxy[0][0][1])\n",
    "    x2 = int(frame_predictions.xyxy[0][0][2])\n",
    "    y2 = int(frame_predictions.xyxy[0][0][3])\n",
    "    \n",
    "    confidence = frame_predictions.xyxy[0][0][4]\n",
    "    \n",
    "    return x1,y1,x2,y2,confidence\n",
    "\n",
    "# crop the id card from the frame for it to be saved or passed to siamese network for evaluating\n",
    "def crop_id_card(frame_predictions, frame):\n",
    "    x1,y1,x2,y2,confidence = get_results_of_yolo(frame_predictions)\n",
    "    # x1 (pixels)  y1 (pixels)  x2 (pixels)  y2 (pixels)   confidence   class\n",
    "    cropped_image = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41369fd2-b0df-4a1e-9b07-b478d32caeb3",
   "metadata": {},
   "source": [
    "### Id card recognition function\n",
    "we need this function to retrieve already saved ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cbf638-26d9-42fc-a30a-7cc7f6bfb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path for id_examples\n",
    "id_examples_filepath = './id_examples/'\n",
    "def retrieve_saved_ids():\n",
    "    saved_ids_labels = []\n",
    "    saved_ids_images = []\n",
    "    for id_img in os.listdir(id_examples_filepath):\n",
    "        id_image_uncropped = cv2.imread(id_examples_filepath+id_img)\n",
    "        saved_ids_images.append(id_image_uncropped)\n",
    "        saved_ids_labels.append(id_img)\n",
    "\n",
    "    id_img_batch = np.array([preprocess_image(image_array) for image_array in saved_ids_images])\n",
    "    if id_img_batch.size == 0:\n",
    "        id_img_batch = np.ones((1, 300, 300, 3), dtype=np.uint8) * 255\n",
    "        saved_ids_labels.append(\"Nothing\")\n",
    "    return id_img_batch, saved_ids_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46146205-58ad-48d6-b160-b218374d8f41",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114c18fe-df28-469c-9b7f-34be394961df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_id_to_database(frame, new_id_name):\n",
    "    # Create a directory to store new ID images if it doesn’t exist\n",
    "    os.makedirs(\"./id_examples/\", exist_ok=True)\n",
    "\n",
    "    # Create a unique file name with ID type and current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path = f\"./id_examples/{new_id_name}_{timestamp}.jpg\"\n",
    "\n",
    "    # Save the image to the specified path\n",
    "    cv2.imwrite(file_path, frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52aa3b08-df6a-4f34-8ab4-5f481db43fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user_for_id_type():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    new_id_type = simpledialog.askstring(\"Input\", \"Enter new ID type name:\")\n",
    "    root.destroy()\n",
    "    return new_id_type\n",
    "    \n",
    "# Create a button that prints the input field's text when clicked\n",
    "def on_button_click(id_image):\n",
    "    new_id_name = input_entry.get()  # Get text from entry field\n",
    "    print(new_id_name + \"es daarqva\")\n",
    "    if new_id_name != None:\n",
    "        add_new_id_to_database(id_image, new_id_name)\n",
    "        id_images_batch, id_labels = retrieve_saved_ids()\n",
    "        input_entry.delete(0, tk.END)  # Clear the entry field after clicking\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_window_submit_window():\n",
    "    window_submit_photo = tk.TK()\n",
    "    window_submit_photo.title(\"ID submission\")\n",
    "    window.geometry(\"1200x800\")\n",
    "    \n",
    "    video_label_submit = tk.Label(window_submit_photo)\n",
    "    video_label_submit.pack()\n",
    "    \n",
    "    input_entry = tk.Entry(window, width=30)\n",
    "    input_entry.pack()\n",
    "\n",
    "    button = tk.Button(window, text=\"Submit\", command=on_button_click)\n",
    "    button.pack()\n",
    "    \n",
    "def check_for_new_id_type(frame,confidence_score):\n",
    "    global frame_counter\n",
    "    if confidence_score > confidence_score_threshold:\n",
    "        if frame_counter < 20:\n",
    "            frame_counter += 1\n",
    "            return False\n",
    "        else:\n",
    "            new_id_name = ask_user_for_id_type()\n",
    "            if new_id_name != None:\n",
    "                add_new_id_to_database(frame, new_id_name)\n",
    "                frame_counter = 0\n",
    "                return True\n",
    "            else:\n",
    "                frame_counter = 0\n",
    "                return False\n",
    "\n",
    "    frame_counter = 0\n",
    "    return False\n",
    "\n",
    "def check_id_page2(image,siamese_conf_score):\n",
    "    if siamese_conf_score > classification_threshold:\n",
    "        new_id_name = ask_user_for_id_type()\n",
    "        if new_id_name != None:\n",
    "            add_new_id_to_database(image, new_id_name)\n",
    "            \n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daab43d-8bf8-41e9-822c-092b2398a29a",
   "metadata": {},
   "source": [
    "# Detecting, segmenting and recognizing descriptive regions in ID cards\n",
    "Part of code for subproject part 2\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def7f02-d340-40a4-a0b7-4e73167cdf61",
   "metadata": {},
   "source": [
    "## Functions checking blurryness and reflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8918911a-9800-4896-9157-ad3e8b9d22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blurry(image, threshold=blurryness_threshold):\n",
    "    \"\"\"\n",
    "    Check if an image is blurry using the variance of the Laplacian.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    threshold (float): Threshold below which the image is considered blurry.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the image is blurry, False otherwise.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return laplacian_var < threshold, laplacian_var\n",
    "\n",
    "def has_reflection(image, bright_threshold=200, reflection_ratio=reflection_threshold):\n",
    "    \"\"\"\n",
    "    Check if an image has significant reflections based on bright regions.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    bright_threshold (int): Pixel intensity threshold to consider as bright.\n",
    "    reflection_ratio (float): Proportion of bright pixels to consider as reflection.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the image has high reflection, False otherwise.\n",
    "    float reflection_percentage\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    bright_pixels = cv2.threshold(gray, bright_threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "    bright_pixel_count = np.sum(bright_pixels == 255)\n",
    "    total_pixels = gray.size\n",
    "    reflection_percentage = bright_pixel_count / total_pixels\n",
    "    return reflection_percentage > reflection_ratio, reflection_percentage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858eebaf-2e81-49b0-8ae1-6a2de9319e05",
   "metadata": {},
   "source": [
    "## Function for extracting faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0971d0-b085-44a9-bdfa-e603fce23ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_face(image):\n",
    "    height, width, dim = image.shape\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    for (x, y, w, h) in faces:\n",
    "        if 5 < ((w*h) * 100)/(height * width) < 50:\n",
    "            image = image[y:y + h, x:x + w]\n",
    "            return image\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588fd3e-9784-400a-8363-bcfde637b9ba",
   "metadata": {},
   "source": [
    "## Sharpening images with convolutional kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52815d77-6edb-45f0-b8eb-1c12de8208bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen_with_kernel(image):\n",
    "    \"\"\"\n",
    "    Sharpen an image using a convolutional kernel.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The sharpened image.\n",
    "    \"\"\"\n",
    "    # Define a sharpening kernel\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "\n",
    "    # Apply the kernel to the image\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    \n",
    "    return sharpened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dbddca-a347-4c27-93dc-254b106d75b4",
   "metadata": {},
   "source": [
    "## signature detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "648a72bb-4ba8-4a22-b6f4-7a0c03c614b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_signature(image):\n",
    "    '''\n",
    "    finds signature based on contours\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The sharpened image.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 47))\n",
    "\n",
    "    # Apply blackhat morphology\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "    \n",
    "    # Threshold and find contours\n",
    "    _, thresh = cv2.threshold(blackhat, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "\n",
    "    # Filter contours for signature area\n",
    "    height, width, dim = image.shape\n",
    "    for c in contours:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if 1.5 < ((w*h) * 100)/(height * width) < 8:\n",
    "            aspect_ratio = w / float(h)\n",
    "    \n",
    "            if 1.5 < aspect_ratio < 6.5: \n",
    "                \n",
    "                # image = image[y:y + h, x:x + w]\n",
    "                image = image[y:y + h, x:x + w]\n",
    "                return image\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62bede-9d0f-4100-b391-d43957c2af5d",
   "metadata": {},
   "source": [
    "## Function for detecting machine readable zone (MRZ) on the back of id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03759594-411e-495d-8f86-3423bd5faf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mrz_area(image):\n",
    "    \"\"\"\n",
    "    Find the area where the MRZ is located in an ID card image.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input ID card image.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: The bounding box (x, y, w, h) of the MRZ area.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize a rectangular and square structuring kernel (this size is dependent on the ID-Card size)\n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 5))\n",
    "    sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 20))\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply GaussianBlur to reduce noise and improve contour detection\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    # Apply blackhat morphological operation to enhance the text\n",
    "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "    \n",
    "    # apply a closing operation using the rectangular kernel to close\n",
    "\t# gaps in between letters\n",
    "    blackhat_closed = cv2.morphologyEx(blackhat, cv2.MORPH_CLOSE, rectKernel)\n",
    "    \n",
    "    # then apply Otsu's thresholding method\n",
    "    thresh = cv2.threshold(blackhat_closed, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # perform another closing operation, this time using the square\n",
    "\t# kernel to close gaps between lines of the MRZ\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)\n",
    "\n",
    "    # Convert the thresholded image to a color image to plot the contours in color\n",
    "    thresh_color = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
    "  \n",
    "\t# find contours in the thresholded image and sort them by their size\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Handle different versions of OpenCV\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "    # Sort contours by area in descending order    \n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    # Initialize ROI as None\n",
    "    roi = None\n",
    "\n",
    "\t# loop over the contours\n",
    "    for i, c in enumerate(cnts):\n",
    "\t\t# compute the bounding box of the contour and use the contour to\n",
    "\t\t# compute the aspect ratio and coverage ratio of the bounding box\n",
    "\t\t# width to the width of the image\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "         # Draw the bounding box on the color thresholded image\n",
    "        color = (0, 255, 0) if i == 0 else (0, 0, 255)  # Green for the first (largest) contour, red for others\n",
    "        cv2.rectangle(thresh_color, (x, y), (x + w, y + h), color, 2)\n",
    "        # cv2.imshow('Thresholded Image with Contours', thresh_color)\n",
    "\n",
    "        ar = w / float(h)\n",
    "        crWidth = w / float(gray.shape[1])\n",
    "\t\t# check to see if the aspect ratio and coverage width are within\n",
    "\t\t# acceptable criteria\n",
    "        if ar > 4 and crWidth > 0.25:\n",
    "            print(\"aris\")\n",
    "\t\t\t# pad the bounding box to have some space for later reading\n",
    "            pad = 5\n",
    "            x = x - pad\n",
    "            y = y - pad\n",
    "            w = w + (pad * 2)\n",
    "            h = h + (pad * 2)\n",
    "\n",
    "\t\t\t# extract the ROI from the image and draw a bounding box\n",
    "\t\t\t# surrounding the MRZ\n",
    "            roi = image[y:y + h, x:x + w]\n",
    "\n",
    "            break\n",
    "    \n",
    "    return (x, y, w, h), roi if roi is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d677e-91e2-48a2-a00d-dab3d494ec1c",
   "metadata": {},
   "source": [
    "## Exctracting text information from id using tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d2ff39d-bba1-41bb-ba6d-dc7b39446cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text(image,lang=\"eng\",config = \"--psm 7 outputbase digits\"):\n",
    "#     text = pytesseract.image_to_string(image, lang=lang,conf)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb24111a-4cae-4965-b5e0-7528f22e5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(cropped_id_card):\n",
    "    blurry, sharpness = is_blurry(cropped_id_card,blurryness_threshold)\n",
    "    reflection, refl_score = has_reflection(cropped_id_card)\n",
    "    print(f'sharpness: {sharpness}, reflection score: {refl_score:.2%}', end='\\r' )\n",
    "    if not blurry and not reflection:\n",
    "        return True, sharpness, refl_score\n",
    "    else:\n",
    "        return False, sharpness, refl_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18298de4-c924-469a-967d-b0ad065763fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "german_20241230_010738.jpg53, reflection score: 14.27%%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chosen_language = \"eng\"\n",
    "is_running = True\n",
    "\n",
    "# Open video capture (0 is the default camera)\n",
    "# video_path = \"./passport_video.mp4\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "frame_counter = 0  # Use local variable instead of global\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps == 0:  # Avoid division by zero\n",
    "    fps = 30  # Default to 30 FPS if unable to retrieve\n",
    "frame_delay = int(1000 / fps) # Calculate frame delay in milliseconds\n",
    "\n",
    "print(frame_delay)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "window = tk.Toplevel()\n",
    "\n",
    "window.title(\"ID Detection\")\n",
    "window.geometry(\"1280x860\")\n",
    "\n",
    "# Configure rows and columns to allow frames to expand\n",
    "window.grid_rowconfigure(0, weight=1)  # Make row 0 expandable\n",
    "window.grid_columnconfigure(0, weight=1) \n",
    "\n",
    "def show_frame(frame):\n",
    "    frame.tkraise()  # Bring the frame to the top\n",
    "    \n",
    "def init_text_label(page,text,row,column):\n",
    "    label = tk.Label(page, text=text, font=(\"Arial\", 12), fg=\"#1c1c1e\")\n",
    "    label.grid(row=row, column=column, padx=5, pady=5, sticky=\"w\") \n",
    "    return label\n",
    "# Function to handle the selection\n",
    "def on_select(option):\n",
    "    global chosen_language\n",
    "    chosen_language = languages[option]\n",
    "    label_lang.config(text=f\"Choosen Language: {chosen_language}\")\n",
    "    \n",
    "def init_dropdown_menu(selected_language,row,column):\n",
    "     # Create a dropdown menu\n",
    "    dropdown = tk.OptionMenu(page2, selected_language, *languages.keys(), command=on_select)\n",
    "    dropdown.config(width=15, font=(\"Arial\", 12))\n",
    "    dropdown.grid(row=row, column=column, padx=5, pady=5, sticky=\"w\") \n",
    "    return dropdown\n",
    "\n",
    "def init_image_label(page,row,column):\n",
    "    # Create a label to display the image\n",
    "    image_label = tk.Label(page)\n",
    "    image_label.grid(row=row, column=column, padx=10, pady=5, sticky=\"w\") \n",
    "    return image_label\n",
    "\n",
    "def show_image(image,image_label):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(image_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    image_label.imgtk = imgtk\n",
    "    image_label.config(image=imgtk)\n",
    "\n",
    "def face_image_resize(face_image):\n",
    "    if face_image is not None:\n",
    "        if face_image.shape[0]>0 and face_image.shape[1]>0:\n",
    "            face_image = cv2.resize(face_image, (200,150))\n",
    "            return face_image\n",
    "\n",
    "def signature_image_resize(signature_image):\n",
    "    if signature_image is not None:\n",
    "        if signature_image.shape[0]>0 and signature_image.shape[1]>0:\n",
    "            # signature_image = cv2.resize(signature_image,(int(signature_image.shape[0] * 1.5), int(signature_image.shape[1] * 1.5)))\n",
    "            return signature_image\n",
    "def show_page_2(image):\n",
    "    global label_lang\n",
    "    page2.tkraise()\n",
    "    \n",
    "    label_prediction = init_text_label(page2, \"prediction for id: \", 0, 2)\n",
    "    \n",
    "    label_lang_text = f\"Choosen Language: {chosen_language}\"\n",
    "    label_lang = init_text_label(page2, label_lang_text, 1, 2)\n",
    "    \n",
    "    # Tkinter variable to store the selected option\n",
    "    selected_language = tk.StringVar(value=\"Select Language\")\n",
    "    \n",
    "     # Create a dropdown menu\n",
    "    dropdown = init_dropdown_menu(selected_language,2,2)\n",
    "    \n",
    "    id_image_label = init_image_label(page2,0,0)\n",
    "    show_image(image,id_image_label)\n",
    "\n",
    "    face_image = find_face(image)\n",
    "    if face_image is not None:\n",
    "        face_image = face_image_resize(face_image)\n",
    "        face_image_label = init_image_label(page2,2,0)\n",
    "        show_image(face_image,face_image_label)\n",
    "\n",
    "    signature_image = find_signature(image)\n",
    "    if signature_image is not None:\n",
    "        signature_image_label = init_image_label(page2,3,0)\n",
    "        signature_image = signature_image_resize(signature_image)\n",
    "        show_image(signature_image,signature_image_label)\n",
    "        \n",
    "    mrz_area, roi = find_mrz_area(image)\n",
    "    if roi is not None:\n",
    "        if roi.shape[0]>0 and roi.shape[1]>0:\n",
    "            mrz_image_label = init_image_label(page2, 4, 0)\n",
    "            show_image(roi,mrz_image_label)\n",
    "            \n",
    "    \n",
    "\n",
    "    prediction_name, confidence_score_siamese = prediction_for_label(image,model)\n",
    "    if prediction_name != \"not found\":\n",
    "        print(prediction_name)\n",
    "        label_prediction.config(text= f'id type: {prediction_name}, {confidence_score_siamese:.2f}')\n",
    "    else:\n",
    "        prediction_name, confidence_score_siamese = prediction_for_label(image,model)\n",
    "        label_prediction.config(text= f'id type: {prediction_name}, {confidence_score_siamese:.2f}')\n",
    "        \n",
    "    window.update()\n",
    "   \n",
    "def prediction_for_label(sharpened_cropped_id, model):\n",
    "    id_images_batch, id_labels = retrieve_saved_ids()\n",
    "    # # Predict with siamese network\n",
    "    prediction_idx, confidence_score_siamese = predict(id_images_batch, sharpened_cropped_id, model)\n",
    "    \n",
    "    if prediction_idx == 999:\n",
    "        prediction_name = \"not found\"\n",
    "        if check_id_page2(sharpened_cropped_id, confidence_score_siamese) == True:\n",
    "            id_images_batch, id_labels = retrieve_saved_ids()\n",
    "    else:\n",
    "        prediction_name = id_labels[prediction_idx]\n",
    "    return prediction_name, confidence_score_siamese\n",
    "    \n",
    "\n",
    "# Create two frames for different pages\n",
    "page1 = tk.Frame(window, bg=\"lightblue\")\n",
    "page2 = tk.Frame(window, bg=\"lightgreen\")\n",
    "page3 = tk.Frame(window)\n",
    "\n",
    "for frame in (page1, page2, page3):\n",
    "    frame.grid(row=0, column=0, sticky=\"nsew\")  # Sticky ensures the frame fills the window\n",
    "\n",
    "# Create a label to display the video feed\n",
    "video_label = init_image_label(page1,0,0)\n",
    "\n",
    "\n",
    "frame_size_x = 1.2\n",
    "video_frame_size = (int(400 * frame_size_x), int(300 * frame_size_x))\n",
    "def on_capture_id_click(cur_sharp, sharp_hist, cur_refl, refl_hist,image):\n",
    "    global is_running\n",
    "    mean_refl = calculate_mean(refl_hist)\n",
    "    mean_sharp = calculate_mean(sharp_hist)\n",
    "    if cur_sharp < mean_sharp:\n",
    "        label_warning = tk.Label(page1, text=f\"image is blurry, please stand still\", font=(\"Arial\", 12))\n",
    "        label_warning.grid(row=1,column=1, padx=10, pady=5, sticky=\"w\") \n",
    "    elif cur_refl < mean_refl:\n",
    "        label_warning = tk.Label(page1, text=f\"Id card has reflections, please get rid of them\", font=(\"Arial\", 12))\n",
    "        label_warning.grid(row=1,column=1, padx=10, pady=5, sticky=\"w\") \n",
    "    else:\n",
    "        is_running = False\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()# Release the camera\n",
    "        show_page_2(image)\n",
    "        \n",
    "        \n",
    "\n",
    "def calculate_mean(hist_list):\n",
    "    return int(sum(hist_list)/len(hist_list))\n",
    "               \n",
    "button_capture_id = tk.Button(page1, text=\"Click Me\", font=(\"Arial\", 12), bg=\"orange\")\n",
    "button_capture_id.grid(row=2, column=0, padx=10, pady=10, sticky=\"w\")  # Align to the left\n",
    "\n",
    "\n",
    "label_info = init_text_label(page1,\"\", 3, 1)\n",
    "\n",
    "# label_text = tk.Label(window, text=\"text from passport\", font=(\"Arial\", 10), fg=\"#1c1c1e\")\n",
    "# label_text.grid(row=0, column=1, padx=10, pady=5) \n",
    "\n",
    "sharpness_hist = []\n",
    "reflection_score_hist = []\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Function to update frame in Tkinter window\n",
    "show_frame(page1)\n",
    "def update_frame():\n",
    "    global frame_counter, id_images_batch, id_labels,is_running\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not is_running:\n",
    "        return  \n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        window.after(10, update_frame)  # Schedule next frame update\n",
    "        return\n",
    "    \n",
    "   \n",
    "    frame = cv2.resize(frame, video_frame_size)\n",
    "    \n",
    "    # Predict bounding box coordinates\n",
    "    bounding_box_prediction = yolo(yolo_model, frame)\n",
    "    \n",
    "    # Make siamese network work only if ID card is found\n",
    "    if bounding_box_prediction.xyxy[0].shape != (0, 6):\n",
    "        # Get coordinates of bounding box\n",
    "        \n",
    "        x1, y1, x2, y2, confidence_score_object_det = get_results_of_yolo(bounding_box_prediction)\n",
    "        \n",
    "        # Crop the ID card\n",
    "        cropped_id_card = crop_id_card(bounding_box_prediction, frame)\n",
    "        is_valid_for_anaysis, sharpness, refl_score = check_validity(cropped_id_card)\n",
    "        sharpness_hist.append(sharpness)\n",
    "        reflection_score_hist.append(refl_score)\n",
    "        if is_valid_for_anaysis:\n",
    "            sharpened_cropped_id = sharpen_with_kernel(cropped_id_card)\n",
    "            if confidence_score_object_det > yolo_confidence_score_threshold:\n",
    "                # Draw bounding box on the frame\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                text = f'{confidence_score_object_det:.2f}'\n",
    "                text_position = (x1, y1 - 10)\n",
    "                cv2.putText(frame, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                button_capture_id.config(command=lambda: on_capture_id_click(sharpness,sharpness_hist,refl_score,reflection_score_hist,sharpened_cropped_id))\n",
    "            \n",
    "        label_info.config(text=f'sharpness: {sharpness}(mean: {calculate_mean(sharpness_hist)}), \\n reflection score: {refl_score:.2%}')\n",
    "        \n",
    "            \n",
    "                \n",
    "                        \n",
    "            \n",
    "                \n",
    "        #     if face_image is not None:\n",
    "        #         result = DeepFace.verify(\n",
    "        #                   img1_path = face_image,\n",
    "        #                   img2_path = \"img2.jpg\",\n",
    "        #                 )\n",
    "                    \n",
    "        #     extracted_text = extract_text(sharpened_cropped_id, lang=chosen_language)\n",
    "        #     label_text.config(text=extracted_text)\n",
    "        #     window.update()\n",
    "        \n",
    "        # # Update the GUI\n",
    "        # window.update()  # Processes pending events\n",
    "    show_image(frame, video_label)\n",
    "    # Convert frame to RGB and update the Tkinter label\n",
    "    # frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # img = Image.fromarray(frame_rgb)\n",
    "    # imgtk = ImageTk.PhotoImage(image=img)\n",
    "    # video_label.imgtk = imgtk\n",
    "    # video_label.configure(image=imgtk)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Schedule the next frame update\n",
    "    video_label.after(frame_delay, update_frame)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "# Start updating frames\n",
    "update_frame()\n",
    "\n",
    "# Define a function to handle window close\n",
    "def on_closing():\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()# Release the camera\n",
    "    window.destroy()  # Close the Tkinter window\n",
    "    \n",
    "# Bind the window close event\n",
    "window.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a854f1-a226-423a-aafb-390b3997d163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mean(sharpness_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "005db7f6-1732-4480-b5e6-380fe81e74ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PRIMER APELLIDO\\n\\nESPAÑOL\\n\\nSEGUNDO APELLIDO\\nESPAÑOL\\nNOMBRE\\n\\nJUAN\\n\\nSEXO NACIONALIDAD\\n\\nM ESP\\n\\nFECHA DE NACIMIENTO\\n\\n1950\\nIDESP\\nAAA023112\\n\\nVALIDO HASTA\\n\\n17 04 2018\\nNO aa\\n'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_text(image, lang=chosen_language):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.fastNlMeansDenoising(image, None, 18, 2, 31)\n",
    "    \n",
    "    cv2.imshow(\"bef\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    text = pytesseract.image_to_string(image, lang='spa+eng')\n",
    "    return text\n",
    "\n",
    "\n",
    "# def extract_name(text):\n",
    "#     name = re.search(r\"PRIMER APELLIDO:\\s*(.*)\", text).group(1)\n",
    "    \n",
    "\n",
    "# surname = re.search(r\"Surname:\\s*(.*)\", text).group(1)\n",
    "# dob = re.search(r\"DOB:\\s*(.*)\", text).group(1)\n",
    "image = cv2.imread(\"./id_examples/spanish_id_1.jpg\")\n",
    "print(image.shape)\n",
    "# Crop region of interest (e.g., for name)\n",
    "image = image[30:280, 150:400]\n",
    "\n",
    "image = cv2.resize(image, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "extracted_text = extract_text(image)\n",
    "extracted_text\n",
    "# cv2.imshow(\"img\", image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a991f0a6-b30b-4c7c-936d-30dc02e77918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ESPAÑOL'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def extract_surname(text,second_surname=False):\n",
    "    \n",
    "    if second_surname == False:\n",
    "        # Define a regex to match the label and extract the value\n",
    "        pattern = r\"PRIMER APELLIDO\\s*\\n+([^\\n]+(?:\\n-s)?)\"\n",
    "    else:\n",
    "        pattern = r\"SEGUNDO APELLIDO\\s*\\n+([^\\n]+(?:\\n-s)?)\"\n",
    "    # Search for the pattern\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        extracted_name = match.group(1).strip()  # Extract the value and clean it\n",
    "        return extracted_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_name(text):\n",
    "    pattern = r\"nNOMBRE\\s*\\n+([^\\n]+(?:\\n-s)?)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "acbd8db7-4c27-47cf-84f2-3d5a96bfa265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_text_fields(image):\n",
    "    image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # gray = cv2.fastNlMeansDenoising(gray, None, 28, 1, 31)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    cv2.imshow(\"blur\", blurred)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    # Use adaptive thresholding to detect edges\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                               cv2.THRESH_BINARY_INV, 25, 25)  \n",
    "\n",
    "    cv2.imshow(\"thresh\", thresh)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Morphological operations to close gaps\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20, 3))\n",
    "    morphed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "\n",
    "    # find contours in the thresholded image and sort them by their size\n",
    "    cnts = cv2.findContours(morphed.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Handle different versions of OpenCV\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "    # Sort contours by area in descending order    \n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    # Draw bounding boxes around detected text fields\n",
    "    # loop over the contours\n",
    "    for i, c in enumerate(cnts):\n",
    "\t\t# compute the bounding box of the contour and use the contour to\n",
    "\t\t# compute the aspect ratio and coverage ratio of the bounding box\n",
    "\t\t# width to the width of the image\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "         # Draw the bounding box on the color thresholded image\n",
    "        # ar = w / float(h)\n",
    "        crWidth = w / float(gray.shape[1])\n",
    "        if crWidth > 0.05:\n",
    "            color = (0, 255, 0) if i == 0 else (0, 0, 255)  # Green for the first (largest) contour, red for others\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        # cv2.imshow('Thresholded Image with Contours', thresh_color)\n",
    "\n",
    "  #       ar = w / float(h)\n",
    "  #       crWidth = w / float(gray.shape[1])\n",
    "\t\t# # check to see if the aspect ratio and coverage width are within\n",
    "\t\t# # acceptable criteria\n",
    "  #       if ar > 2 and crWidth > 0.5:\n",
    "  #           # print(\"aris\")\n",
    "\t\t# \t# pad the bounding box to have some space for later reading\n",
    "  #           pad = 5\n",
    "  #           x = x - pad\n",
    "  #           y = y - pad\n",
    "  #           w = w + (pad * 2)\n",
    "  #           h = h + (pad * 2)\n",
    "\n",
    "\t\t# \t# extract the ROI from the image and draw a bounding box\n",
    "\t\t# \t# surrounding the MRZ\n",
    "  #           roi = image[y:y + h, x:x + w]\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "    # Save the output image\n",
    "    cv2.imshow(\"fields\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f36412bc-f078-4ad5-ac04-7d299ab2b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 600, 3)\n",
      "PRIMER APELLIDO\n",
      "\n",
      "ESPAÑOL\n",
      "\n",
      "SEGUNDO APELLIDO\n",
      "ESPAÑOL\n",
      "NOMBRE\n",
      "\n",
      "JUAN\n",
      "\n",
      "SEXO NACIONALIDAD\n",
      "\n",
      "M ESP\n",
      "\n",
      "FECHA DE NACIMIENTO\n",
      "\n",
      "1950\n",
      "IDESP\n",
      "AAA023112\n",
      "\n",
      "VALIDO HASTA\n",
      "\n",
      "17 04 2018\n",
      "NO aa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"./id_examples/spanish_id_1.jpg\")\n",
    "print(image.shape)\n",
    "# Crop region of interest (e.g., for name)\n",
    "image = image[30:280, 150:400]\n",
    "\n",
    "image = cv2.resize(image, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "extracted_text = extract_text(image)\n",
    "print(extracted_text)\n",
    "cv2.imshow(\"img\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e739b2d2-f875-4347-b0ac-469fff851b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_text_fields(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d551d511-1085-4c96-a686-a28198fd51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50306ab6-5838-427c-888f-c2bcbcfae8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kk():\n",
    "    return b + 5 \n",
    "def kkk():\n",
    "    global b\n",
    "    b = 5\n",
    "    c = kk()\n",
    "    return a + c\n",
    "a = 15\n",
    "\n",
    "kkk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b328cd6-0066-4674-bc2a-7dfbd26686ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
