{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0530cf7-be8f-417d-ae19-2f0b7c3c23ea",
   "metadata": {},
   "source": [
    "# Start\n",
    "these libraries are neccesary to install to use this code. specific versions must be installed. just run the code below for the first time when you will run this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a23dbb-93be-44e5-9d2d-b37db1ec3108",
   "metadata": {},
   "source": [
    "## Important variables to set before running code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecef956-02f5-476a-90ba-eafdfd40aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set those thresholds based on your needs\n",
    "blurryness_threshold = 50\n",
    "reflection_threshold = 0.05\n",
    "yolo_confidence_score_threshold = 0.8\n",
    "classification_threshold = 0.95\n",
    "similarity_score = 0\n",
    "chosen_language = \"eng+spa\"\n",
    "model_name = f'MobileNetV3_1024_siamese.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5794efd-38da-4e94-aa8a-87ad915d18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pillow==10.4.0\n",
    "# !pip install tensorflow==2.8.3\n",
    "# !pip install ultralytics==8.3.23\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install opencv-python==4.10.0.84\n",
    "# !pip install numpy==1.26.4\n",
    "# !pip install pytesseract==0.3.13\n",
    "# !pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c722543-a20d-4fda-9931-9461f2e6892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras import backend as K\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from tkinter import simpledialog, filedialog\n",
    "import pytesseract \n",
    "from deepface import DeepFace\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Clear last session to feee up space before new\n",
    "K.clear_session()\n",
    "\n",
    "# if GPU is available this code will state 1\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# image height and width for input into Siamese Network with MobileNetV3 as a base \n",
    "IMG_HEIGHT = 300\n",
    "IMG_WIDTH = 300\n",
    "# important for tesseract to work in jupyter notebook\n",
    "# PATH is relative chooose the one where tesseract.exe is installed\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe' \n",
    "\n",
    "\n",
    "# Options for the dropdown menu\n",
    "languages = {\"Georgian\":\"kat\", \"French\": \"fra\", \"German\": \"deu\", \"Spanish\" :\"spa\", \"Russian\":\"rus\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d057d105-8b2f-4703-bcf4-03bd37f68a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_im(image):\n",
    "    cv2.imshow(\"h\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29116eb-92d0-4de3-be66-80b431f45fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e5fa42f-07fc-44e2-a40f-35f4c56092c9",
   "metadata": {},
   "source": [
    "# AI code part for object detection and id card recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87744d82-8f01-420c-82a8-0513700c7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Siamese network model file\n",
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model\n",
    "\n",
    "# Preprocessing function to resize and normalize image\n",
    "def preprocess_image(image):\n",
    "    # resize image to match height and width specified earlier \n",
    "    image_resized = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    \n",
    "    # Normalize values of pixels to be around 0.0-1.0\n",
    "    image_resized = image_resized / 255.0  \n",
    "    \n",
    "    return image_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2cadd-d7b3-443e-9aba-5fc5a79e95ec",
   "metadata": {},
   "source": [
    "## Id card type prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51bd57ed-f88e-448b-bfce-8e14b644266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese network prediction function  \n",
    "def predict(id_images, img_to_evaluate, model, threshold = classification_threshold):\n",
    "    '''\n",
    "    id_images - batch of saved id images, should be numpy array of shape (x, 300, 300, 3) where x is number of saved images \n",
    "    or path for image which is saved in the memory and should be compared to, in this case it is string path\n",
    "    img_to_evaluate - image which will be compared to img_base which is saved in the memory\n",
    "    model - AI model file which contains trained weights for siamese network\n",
    "    threshold - optional value which we can state for siamese networks confidence score from 0.50 till 0.99, which means \n",
    "    that results will be shown only if confidence score for similarity is more than threshold\n",
    "    '''\n",
    "\n",
    "        \n",
    "    # preprocess images\n",
    "    if type(id_images) == str:\n",
    "        img_base = np.array(preprocess_image(id_images))\n",
    "        img_base = np.expand_dims(img_base, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    elif isinstance(id_images, np.ndarray):\n",
    "        img_base = id_images\n",
    "\n",
    "    \n",
    "    img_to_evaluate = np.array(preprocess_image(img_to_evaluate))\n",
    "\n",
    "    # expand dimensions of images to add batch dimension \n",
    "    \n",
    "    img_to_evaluate = np.expand_dims(img_to_evaluate, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    \n",
    "    img_to_evaluate = np.tile(img_to_evaluate, (len(img_base), 1, 1, 1))\n",
    "    \n",
    "    # predict the similarity between the two images,\n",
    "    # returns tensor with two values like this [0.37, 0.63] which states how similar are images\n",
    "    # 0.0 no similarity , 1.0 full similarity\n",
    "    preds = model.predict((img_base,img_to_evaluate))\n",
    "    predicted_idx = np.argmax(preds[:, 1])\n",
    "\n",
    "    if preds[predicted_idx][1] > threshold:\n",
    "        return predicted_idx, preds[predicted_idx][1]\n",
    "    else:\n",
    "        return 999,999\n",
    "    # if the model is sure that shown images are more similar than threshold it will output the 1, if not 0\n",
    "    # if predictions are less than a threshold it will output 2   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78faf1-1e67-4345-b0e4-1b702dee5834",
   "metadata": {},
   "source": [
    "### Load models for Object detection and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aaa0955-860a-436d-921f-eb6d80b6f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Saba/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-1-20 Python-3.9.21 torch-2.5.0+cu118 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# load the siamese network model\n",
    "\n",
    "model = load_model(model_name)\n",
    "# load the YOLO model \n",
    "yolo_model = torch.hub.load('ultralytics/yolov5',\"custom\", path=r'.\\yolov5\\runs\\train\\exp6\\weights/best.pt',force_reload=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e51b31-05a5-4ce7-9540-a548550f6a65",
   "metadata": {},
   "source": [
    "## YOLO object detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddec8c6-3b4a-4944-9808-293ab073cfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "# function returns YOLO model object detection results \n",
    "def yolo(yolo_model,frame):\n",
    "    results = yolo_model(frame)\n",
    "    return results\n",
    "\n",
    "kk = cv2.imread(\"./id_examples/spanish_id_1.jpg\")\n",
    "kk = cv2.resize(kk,(300,300))\n",
    "print(kk.shape)\n",
    "kkk = yolo(yolo_model,kk)\n",
    "predict(np.array([kk]),kk,model)\n",
    "del kk,kkk\n",
    "# unpack the coordinates of bounding boxes and confidence score for ID class\n",
    "def get_results_of_yolo(frame_predictions):\n",
    "    x1 = int(frame_predictions.xyxy[0][0][0])\n",
    "    y1 = int(frame_predictions.xyxy[0][0][1])\n",
    "    x2 = int(frame_predictions.xyxy[0][0][2])\n",
    "    y2 = int(frame_predictions.xyxy[0][0][3])\n",
    "    \n",
    "    confidence = frame_predictions.xyxy[0][0][4]\n",
    "    \n",
    "    return x1,y1,x2,y2,confidence\n",
    "\n",
    "# crop the id card from the frame for it to be saved or passed to siamese network for evaluating\n",
    "def crop_id_card(frame_predictions, frame):\n",
    "    x1,y1,x2,y2,confidence = get_results_of_yolo(frame_predictions)\n",
    "    # x1 (pixels)  y1 (pixels)  x2 (pixels)  y2 (pixels)   confidence   class\n",
    "    cropped_image = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41369fd2-b0df-4a1e-9b07-b478d32caeb3",
   "metadata": {},
   "source": [
    "### Id card recognition function\n",
    "we need this function to retrieve already saved ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60cbf638-26d9-42fc-a30a-7cc7f6bfb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path for id_examples\n",
    "id_examples_filepath = './id_examples/'\n",
    "def retrieve_saved_ids():\n",
    "    saved_ids_labels = []\n",
    "    saved_ids_images = []\n",
    "    for id_img in os.listdir(id_examples_filepath):\n",
    "        id_image_uncropped = cv2.imread(id_examples_filepath+id_img)\n",
    "        saved_ids_images.append(id_image_uncropped)\n",
    "        saved_ids_labels.append(id_img)\n",
    "\n",
    "    id_img_batch = np.array([preprocess_image(image_array) for image_array in saved_ids_images])\n",
    "    if id_img_batch.size == 0:\n",
    "        id_img_batch = np.ones((1, 300, 300, 3), dtype=np.uint8) * 255\n",
    "        saved_ids_labels.append(\"Nothing\")\n",
    "    return id_img_batch, saved_ids_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46146205-58ad-48d6-b160-b218374d8f41",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114c18fe-df28-469c-9b7f-34be394961df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_id_to_database(frame, new_id_name):\n",
    "    # Create a directory to store new ID images if it doesnâ€™t exist\n",
    "    os.makedirs(\"./id_examples/\", exist_ok=True)\n",
    "\n",
    "    # Create a unique file name with ID type and current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path = f\"./id_examples/{new_id_name}_{timestamp}.jpg\"\n",
    "\n",
    "    # Save the image to the specified path\n",
    "    cv2.imwrite(file_path, frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52aa3b08-df6a-4f34-8ab4-5f481db43fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user_for_id_type():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    new_id_type = simpledialog.askstring(\"Input\", \"Enter new ID type name:\")\n",
    "    root.destroy()\n",
    "    return new_id_type\n",
    "    \n",
    "# Create a button that prints the input field's text when clicked\n",
    "def on_button_click(id_image):\n",
    "    new_id_name = input_entry.get()  # Get text from entry field\n",
    "    print(new_id_name + \"es daarqva\")\n",
    "    if new_id_name != None:\n",
    "        add_new_id_to_database(id_image, new_id_name)\n",
    "        id_images_batch, id_labels = retrieve_saved_ids()\n",
    "        input_entry.delete(0, tk.END)  # Clear the entry field after clicking\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_window_submit_window():\n",
    "    window_submit_photo = tk.TK()\n",
    "    window_submit_photo.title(\"ID submission\")\n",
    "    window.geometry(\"1200x800\")\n",
    "    \n",
    "    video_label_submit = tk.Label(window_submit_photo)\n",
    "    video_label_submit.pack()\n",
    "    \n",
    "    input_entry = tk.Entry(window, width=30)\n",
    "    input_entry.pack()\n",
    "\n",
    "    button = tk.Button(window, text=\"Submit\", command=on_button_click)\n",
    "    button.pack()\n",
    "    \n",
    "def check_for_new_id_type(frame,confidence_score):\n",
    "    global frame_counter\n",
    "    if confidence_score > confidence_score_threshold:\n",
    "        if frame_counter < 20:\n",
    "            frame_counter += 1\n",
    "            return False\n",
    "        else:\n",
    "            new_id_name = ask_user_for_id_type()\n",
    "            if new_id_name != None:\n",
    "                add_new_id_to_database(frame, new_id_name)\n",
    "                frame_counter = 0\n",
    "                return True\n",
    "            else:\n",
    "                frame_counter = 0\n",
    "                return False\n",
    "\n",
    "    frame_counter = 0\n",
    "    return False\n",
    "\n",
    "def check_id_page2(image,siamese_conf_score):\n",
    "    if siamese_conf_score > classification_threshold:\n",
    "        new_id_name = ask_user_for_id_type()\n",
    "        if new_id_name != None:\n",
    "            add_new_id_to_database(image, new_id_name)\n",
    "            \n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daab43d-8bf8-41e9-822c-092b2398a29a",
   "metadata": {},
   "source": [
    "# Detecting, segmenting and recognizing descriptive regions in ID cards\n",
    "Part of code for subproject part 2\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def7f02-d340-40a4-a0b7-4e73167cdf61",
   "metadata": {},
   "source": [
    "## Functions checking blurryness and reflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8918911a-9800-4896-9157-ad3e8b9d22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blurry(image, threshold=blurryness_threshold):\n",
    "    \"\"\"\n",
    "    Check if an image is blurry using the variance of the Laplacian.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    threshold (float): Threshold below which the image is considered blurry.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the image is blurry, False otherwise.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return laplacian_var < threshold, laplacian_var\n",
    "\n",
    "def has_reflection(image, bright_threshold=200, reflection_ratio=reflection_threshold):\n",
    "    \"\"\"\n",
    "    Check if an image has significant reflections based on bright regions.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    bright_threshold (int): Pixel intensity threshold to consider as bright.\n",
    "    reflection_ratio (float): Proportion of bright pixels to consider as reflection.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the image has high reflection, False otherwise.\n",
    "    float reflection_percentage\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    bright_pixels = cv2.threshold(gray, bright_threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "    bright_pixel_count = np.sum(bright_pixels == 255)\n",
    "    total_pixels = gray.size\n",
    "    reflection_percentage = bright_pixel_count / total_pixels\n",
    "    return reflection_percentage > reflection_ratio, reflection_percentage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858eebaf-2e81-49b0-8ae1-6a2de9319e05",
   "metadata": {},
   "source": [
    "## Function for extracting faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec0971d0-b085-44a9-bdfa-e603fce23ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_face(image):\n",
    "    height, width, dim = image.shape\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    face_cascade = None\n",
    "    del face_cascade\n",
    "    for (x, y, w, h) in faces:\n",
    "        if 5 < ((w*h) * 100)/(height * width) < 50:\n",
    "            image = image[y:y + h, x:x + w]\n",
    "            return image\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588fd3e-9784-400a-8363-bcfde637b9ba",
   "metadata": {},
   "source": [
    "## Sharpening images with convolutional kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52815d77-6edb-45f0-b8eb-1c12de8208bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen_with_kernel(image):\n",
    "    \"\"\"\n",
    "    Sharpen an image using a convolutional kernel.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The sharpened image.\n",
    "    \"\"\"\n",
    "    # Define a sharpening kernel\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "\n",
    "    # Apply the kernel to the image\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    \n",
    "    return sharpened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dbddca-a347-4c27-93dc-254b106d75b4",
   "metadata": {},
   "source": [
    "## signature detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "648a72bb-4ba8-4a22-b6f4-7a0c03c614b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_signature(image):\n",
    "    '''\n",
    "    finds signature based on contours\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The sharpened image.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 47))\n",
    "\n",
    "    # Apply blackhat morphology\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "    \n",
    "    # Threshold and find contours\n",
    "    _, thresh = cv2.threshold(blackhat, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "\n",
    "    # Filter contours for signature area\n",
    "    height, width, dim = image.shape\n",
    "    for c in contours:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if 1.5 < ((w*h) * 100)/(height * width) < 8:\n",
    "            aspect_ratio = w / float(h)\n",
    "    \n",
    "            if 1.5 < aspect_ratio < 6.5: \n",
    "                \n",
    "                # image = image[y:y + h, x:x + w]\n",
    "                image = image[y:y + h, x:x + w]\n",
    "                return image\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62bede-9d0f-4100-b391-d43957c2af5d",
   "metadata": {},
   "source": [
    "## Function for detecting machine readable zone (MRZ) on the back of id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03759594-411e-495d-8f86-3423bd5faf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mrz_area(image):\n",
    "    \"\"\"\n",
    "    Find the area where the MRZ is located in an ID card image.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input ID card image.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: The bounding box (x, y, w, h) of the MRZ area.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize a rectangular and square structuring kernel (this size is dependent on the ID-Card size)\n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 5))\n",
    "    sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 20))\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply GaussianBlur to reduce noise and improve contour detection\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    # Apply blackhat morphological operation to enhance the text\n",
    "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "    \n",
    "    # apply a closing operation using the rectangular kernel to close\n",
    "\t# gaps in between letters\n",
    "    blackhat_closed = cv2.morphologyEx(blackhat, cv2.MORPH_CLOSE, rectKernel)\n",
    "    \n",
    "    # then apply Otsu's thresholding method\n",
    "    thresh = cv2.threshold(blackhat_closed, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # perform another closing operation, this time using the square\n",
    "\t# kernel to close gaps between lines of the MRZ\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)\n",
    "\n",
    "    # Convert the thresholded image to a color image to plot the contours in color\n",
    "    thresh_color = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
    "  \n",
    "\t# find contours in the thresholded image and sort them by their size\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Handle different versions of OpenCV\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "    # Sort contours by area in descending order    \n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    # Initialize ROI as None\n",
    "    roi = None\n",
    "\n",
    "\t# loop over the contours\n",
    "    for i, c in enumerate(cnts):\n",
    "\t\t# compute the bounding box of the contour and use the contour to\n",
    "\t\t# compute the aspect ratio and coverage ratio of the bounding box\n",
    "\t\t# width to the width of the image\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "         # Draw the bounding box on the color thresholded image\n",
    "        color = (0, 255, 0) if i == 0 else (0, 0, 255)  # Green for the first (largest) contour, red for others\n",
    "        cv2.rectangle(thresh_color, (x, y), (x + w, y + h), color, 2)\n",
    "        # cv2.imshow('Thresholded Image with Contours', thresh_color)\n",
    "\n",
    "        ar = w / float(h)\n",
    "        crWidth = w / float(gray.shape[1])\n",
    "\t\t# check to see if the aspect ratio and coverage width are within\n",
    "\t\t# acceptable criteria\n",
    "        if ar > 4 and crWidth > 0.25:\n",
    "            print(\"aris\")\n",
    "\t\t\t# pad the bounding box to have some space for later reading\n",
    "            pad = 5\n",
    "            x = x - pad\n",
    "            y = y - pad\n",
    "            w = w + (pad * 2)\n",
    "            h = h + (pad * 2)\n",
    "\n",
    "\t\t\t# extract the ROI from the image and draw a bounding box\n",
    "\t\t\t# surrounding the MRZ\n",
    "            roi = image[y:y + h, x:x + w]\n",
    "\n",
    "            break\n",
    "    \n",
    "    return (x, y, w, h), roi if roi is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d677e-91e2-48a2-a00d-dab3d494ec1c",
   "metadata": {},
   "source": [
    "## Exctracting text information from id using tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d2ff39d-bba1-41bb-ba6d-dc7b39446cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text(image,lang=\"eng\",config = \"--psm 7 outputbase digits\"):\n",
    "#     text = pytesseract.image_to_string(image, lang=lang,conf)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb24111a-4cae-4965-b5e0-7528f22e5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(cropped_id_card):\n",
    "    blurry, sharpness = is_blurry(cropped_id_card,blurryness_threshold)\n",
    "    reflection, refl_score = has_reflection(cropped_id_card)\n",
    "    print(f'sharpness: {sharpness}, reflection score: {refl_score:.2%}', end='\\r' )\n",
    "    if not blurry and not reflection:\n",
    "        return True, sharpness, refl_score\n",
    "    else:\n",
    "        return False, sharpness, refl_score\n",
    "\n",
    "\n",
    "def match_search(text,pattern):\n",
    "    # Search for the pattern\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        extracted_info = match.group(1).strip()  # Extract the value and clean it\n",
    "        return extracted_info\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def extract_surname(text,second_surname=False):\n",
    "    if second_surname == False:\n",
    "        # Define a regex to match the label and extract the value\n",
    "        pattern = r\"PRIMER APELLIDO\\s*\\n+([^\\n]+(?:\\n-s)?)\"\n",
    "    else:\n",
    "        pattern = r\"SEGUNDO APELLIDO\\s*\\n+([^\\n]+(?:\\n-s)?)\"\n",
    "        \n",
    "    return match_search(text,pattern)\n",
    "\n",
    "\n",
    "def extract_name(text):\n",
    "    pattern = r\"NOMBRE\\s*\\n+([^\\n]+(?:\\n-s)?)\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    return match_search(text,pattern)\n",
    "\n",
    "def extract_date_of_birth(text):\n",
    "    pattern = r\"FECHA DE NACIMIENTO\\.?\\s*\\n+(\\d{2}\\s\\d{2}\\s\\d{4})\"\n",
    "    \n",
    "    return match_search(text,pattern)\n",
    "def extract_expiration_date(text):\n",
    "    pattern = r\"VALIDO HASTA\\.?\\s*\\n+(\\d{2}\\s\\d{2}\\s\\d{4})\"\n",
    "    \n",
    "    return match_search(text,pattern)\n",
    "\n",
    "def extract_dni(image):\n",
    "    text = extract_text(image)\n",
    "    text = text.strip().split('\\n')\n",
    "    return text[-1]\n",
    "\n",
    "def extract_text(image, lang=chosen_language):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.fastNlMeansDenoising(image, None, 18, 2, 31)\n",
    "    \n",
    "    return pytesseract.image_to_string(image, lang='spa+eng')\n",
    "\n",
    "def crop_image_for_data(image,x1,x2,y1,y2): \n",
    "    return image[x1:x2, y1:y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18298de4-c924-469a-967d-b0ad065763fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spa_20250106_200959.jpg24376, reflection score: 27.70%%\n",
      "(380, 600, 3)\n",
      "ESPAROL\n",
      "ESPAROL\n",
      "\n",
      "Taw\n",
      "\n",
      "axe eaomamoas\n",
      "LJ Esp\n",
      "ead\n",
      "10 01 1950\n",
      "oe\n",
      "\n",
      "AAAQ231 \"2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "is_running = True\n",
    "is_running_face = True\n",
    "face_image_from_id = None\n",
    "live_face = None\n",
    "# Open video capture (0 is the default camera)\n",
    "# video_path = \"./passport_video.mp4\"\n",
    "cam = 0\n",
    "cap = cv2.VideoCapture(cam)\n",
    "\n",
    "frame_counter = 0  # Use local variable instead of global\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps == 0:  # Avoid division by zero\n",
    "    fps = 30  # Default to 30 FPS if unable to retrieve\n",
    "frame_delay = int(1000 / fps) # Calculate frame delay in milliseconds\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "window = tk.Toplevel()\n",
    "\n",
    "window.title(\"ID Detection\")\n",
    "window.geometry(\"1280x720\")\n",
    "\n",
    "# Configure rows and columns to allow frames to expand\n",
    "window.grid_rowconfigure(0, weight=1)  # Make row 0 expandable\n",
    "window.grid_columnconfigure(0, weight=1) \n",
    "\n",
    "def show_frame(frame):\n",
    "    frame.tkraise()  # Bring the frame to the top\n",
    "\n",
    "def init_text_label(page,text,row,column):\n",
    "    label = tk.Label(page, text=text, font=(\"Arial\", 12), fg=\"#1c1c1e\")\n",
    "    label.grid(row=row, column=column, padx=5, pady=5, sticky=\"w\") \n",
    "    return label\n",
    "\n",
    "# Function to handle the selection language on dropdown\n",
    "def on_select(option):\n",
    "    global chosen_language\n",
    "    chosen_language = languages[option]\n",
    "    label_lang.config(text=f\"Choosen Language: {chosen_language}\")\n",
    "\n",
    "\n",
    "# Initializing dropdown menu \n",
    "def init_dropdown_menu(selected_language,row,column):\n",
    "     # Create a dropdown menu\n",
    "    dropdown = tk.OptionMenu(page2, selected_language, *languages.keys(), command=on_select)\n",
    "    dropdown.config(width=15, font=(\"Arial\", 12))\n",
    "    dropdown.grid(row=row, column=column, padx=5, pady=5, sticky=\"w\") \n",
    "    return dropdown\n",
    "\n",
    "def init_slider(page, row, column, scale_from, scale_to, label_text,initial_value, resolution):\n",
    "    horizontal_slider = tk.Scale(page,\n",
    "                                 from_=scale_from,\n",
    "                                 to=scale_to,\n",
    "                                 orient='horizontal',\n",
    "                                 label=label_text,\n",
    "                                 resolution=resolution,\n",
    "                                length=200,       # Length of the slider (in pixels)\n",
    "                                sliderlength=20,  # Length of the slider knob\n",
    "                                width=10 )\n",
    "    horizontal_slider.grid(row=row, column=column, padx=10, pady=5, sticky=\"w\") \n",
    "    horizontal_slider.set(initial_value)\n",
    "    return horizontal_slider\n",
    "# Initializing image label on the page\n",
    "def init_image_label(page,row,column):\n",
    "    # Create a label to display the image\n",
    "    image_label = tk.Label(page)\n",
    "    image_label.grid(row=row, column=column, padx=10, pady=5, sticky=\"w\") \n",
    "    return image_label\n",
    "\n",
    "# function for showing image on video and image labels\n",
    "def show_image(image,image_label):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(image_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    image_label.imgtk = imgtk\n",
    "    image_label.config(image=imgtk)\n",
    "    \n",
    "def show_value(val):\n",
    "    label.config(text=f\"Slider Value: {val}\")\n",
    "\n",
    "def face_image_resize(face_image):\n",
    "    if face_image is not None:\n",
    "        if face_image.shape[0]>0 and face_image.shape[1]>0:\n",
    "            face_image = cv2.resize(face_image, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_CUBIC)\n",
    "            return face_image\n",
    "\n",
    "def signature_image_resize(signature_image):\n",
    "    if signature_image is not None:\n",
    "        if signature_image.shape[0]>0 and signature_image.shape[1]>0:\n",
    "            # signature_image = cv2.resize(signature_image,(int(signature_image.shape[0] * 1.5), int(signature_image.shape[1] * 1.5)))\n",
    "            return signature_image\n",
    "\n",
    "def show_page_2(image):\n",
    "    global label_lang, face_image_from_id,button_compare, button_capture_face,button_analyse\n",
    "    page2.tkraise()\n",
    "    \n",
    "    label_prediction = init_text_label(page2, \"prediction for id: \", 0, 2)\n",
    "    \n",
    "    label_lang_text = f\"Choosen Language: {chosen_language}\"\n",
    "    label_lang = init_text_label(page2, label_lang_text, 1, 2)\n",
    "    \n",
    "    # Tkinter variable to store the selected option\n",
    "    selected_language = tk.StringVar(value=\"Select Language\")\n",
    "    \n",
    "    # Create a dropdown menu\n",
    "    dropdown = init_dropdown_menu(selected_language,2,2)\n",
    "    \n",
    "    id_image_label = init_image_label(page2,0,0)\n",
    "    show_image(image,id_image_label)\n",
    "\n",
    "    face_image = find_face(image)\n",
    "    if face_image is not None:\n",
    "        face_image = face_image_resize(face_image)\n",
    "        face_image_from_id = face_image\n",
    "        face_image_label = init_image_label(page2,2,0)\n",
    "        show_image(face_image,face_image_label)\n",
    "\n",
    "    signature_image = find_signature(image)\n",
    "    if signature_image is not None:\n",
    "        signature_image_label = init_image_label(page2,3,0)\n",
    "        signature_image = signature_image_resize(signature_image)\n",
    "        show_image(signature_image,signature_image_label)\n",
    "        \n",
    "    mrz_area, roi = find_mrz_area(image)\n",
    "    if roi is not None:\n",
    "        if roi.shape[0]>0 and roi.shape[1]>0:\n",
    "            mrz_image_label = init_image_label(page2, 4, 0)\n",
    "            show_image(roi,mrz_image_label)\n",
    "            \n",
    "    prediction_name, confidence_score_siamese = prediction_for_label(image,model)\n",
    "    if prediction_name != \"not found\":\n",
    "        print(prediction_name)\n",
    "        if \"spanish\" in prediction_name:\n",
    "            label_lang = \"spa+eng\"\n",
    "        label_prediction.config(text= f'id type: {prediction_name}, {confidence_score_siamese:.2f}')\n",
    "    else:\n",
    "        prediction_name, confidence_score_siamese = prediction_for_label(image,model)\n",
    "        label_prediction.config(text= f'id type: {prediction_name}, {confidence_score_siamese:.2f}')\n",
    "\n",
    "    \n",
    "    image_resized_data = cv2.resize(image,(600, 380), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    image_crop_data = crop_image_for_data(image_resized_data, 30, 280, 150, 400)\n",
    "    image_crop_data_resized = cv2.resize(image_crop_data, None, fx=1.2, fy=1.2, interpolation=cv2.INTER_CUBIC)\n",
    "    print(image_resized_data.shape)\n",
    "    # show_im(image_crop_data)\n",
    "    image_crop_dni = crop_image_for_data(image_resized_data,300,380,0,200)\n",
    "    image_crop_dni_resized = cv2.resize(image_crop_dni, None, fx=1.2, fy=1.2, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    extracted_text = extract_text(image_crop_data_resized)\n",
    "    print(extracted_text)\n",
    "    first_surname = extract_surname(extracted_text)\n",
    "    second_surname = extract_surname(extracted_text,second_surname=True)\n",
    "    name = extract_name(extracted_text)\n",
    "    date_of_birth = extract_date_of_birth(extracted_text)\n",
    "    expiration_date = extract_expiration_date(extracted_text)\n",
    "    dni = extract_dni(image_crop_dni_resized)\n",
    "    full_text = str(first_surname) + str(second_surname) + str(name) + str(date_of_birth) + str(expiration_date) + str(dni)\n",
    "    label_data = init_text_label(page2, full_text, 3, 2)\n",
    "    button_save_data = tk.Button(page2, text=\"save to database\", font=(\"Arial\", 12), bg=\"orange\")\n",
    "    button_save_data.grid(row=5,column=4,padx=10, pady=10, sticky=\"w\")\n",
    "    \n",
    "    global similarity_score\n",
    "    button_save_data.config(command=lambda: save_file(\"new_file\", first_surname, second_surname, name, date_of_birth, expiration_date, dni, similarity_score))\n",
    "    button_compare = tk.Button(page2, text=\"open camera for face recognition\", font=(\"Arial\", 12), bg=\"orange\")\n",
    "    button_compare.grid(row=1, column=4, padx=10, pady=10, sticky=\"w\")  # Align to the left\n",
    "    button_compare.config(command=lambda: open_cap_face())\n",
    "    \n",
    "    button_capture_face = tk.Button(page2, text=\"capture\", font=(\"Arial\", 12), bg=\"orange\")\n",
    "    button_capture_face.grid(row=3, column=4, padx=10, pady=10, sticky=\"w\")  # Align to the left\n",
    "    button_analyse = tk.Button(page2, text=\"analyse\", font=(\"Arial\", 12), bg=\"orange\")\n",
    "    button_analyse.grid(row=4, column=4, padx=10, pady=10, sticky=\"w\")  # Align to the left\n",
    "    \n",
    "    window.update()\n",
    "\n",
    "def structure_data(first_surname=\"none\", second_surname=\"none\", name=\"none\", date_of_birth=\"none\", expiration_date=\"none\", dni=\"none\",similarity_score=\"none\"):\n",
    "    full_data = {\"first_surname\":first_surname,\n",
    "                 \"second_surname\":second_surname,\n",
    "                 \"name\":name,\n",
    "                 \"date_of_birth\":date_of_birth,\n",
    "                 \"expiration_date\":expiration_date,\n",
    "                 \"dni\":dni,\n",
    "                \"similarity_score\":similarity_score}\n",
    "    \n",
    "    return full_data\n",
    "    \n",
    "def save_file(filename,first_surname, second_surname, name, date_of_birth, expiration_date, dni,similarity):\n",
    "    data = structure_data(first_surname, second_surname, name, date_of_birth, expiration_date, dni,similarity)\n",
    "    folder = \"./database/\"\n",
    "    filename = filename+\".json\"\n",
    "    file_path = folder+filename\n",
    "    # Save JSON to file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "        \n",
    "def prediction_for_label(sharpened_cropped_id, model):\n",
    "    id_images_batch, id_labels = retrieve_saved_ids()\n",
    "    # # Predict with siamese network\n",
    "    prediction_idx, confidence_score_siamese = predict(id_images_batch, sharpened_cropped_id, model)\n",
    "    \n",
    "    if prediction_idx == 999:\n",
    "        prediction_name = \"not found\"\n",
    "        if check_id_page2(sharpened_cropped_id, confidence_score_siamese) == True:\n",
    "            id_images_batch, id_labels = retrieve_saved_ids()\n",
    "    else:\n",
    "        prediction_name = id_labels[prediction_idx]\n",
    "    return prediction_name, confidence_score_siamese\n",
    "    \n",
    "\n",
    "# Create two frames for different pages\n",
    "page1 = tk.Frame(window, bg=\"lightblue\")\n",
    "page2 = tk.Frame(window, bg=\"lightgreen\")\n",
    "page3 = tk.Frame(window)\n",
    "\n",
    "for frame in (page1, page2, page3):\n",
    "    frame.grid(row=0, column=0, sticky=\"nsew\")  # Sticky ensures the frame fills the window\n",
    "\n",
    "# Create a label to display the video feed\n",
    "video_label = init_image_label(page1,0,0)\n",
    "video_label_face = init_image_label(page2, 0, 4)\n",
    "\n",
    "def change_blurryness(val):\n",
    "    global blurryness_threshold\n",
    "    blurryness_threshold = val\n",
    "    \n",
    "def change_reflection(val):\n",
    "    global reflection_threshold\n",
    "    reflection_threshold = val\n",
    "\n",
    "def change_yolo_confidence(val):\n",
    "    global yolo_confidence_score_threshold\n",
    "    yolo_confidence_score_threshold = val\n",
    "    \n",
    "def change_classification_threshold(val):\n",
    "    global classification_threshold\n",
    "    classification_threshold = val\n",
    "\n",
    "blurryness_slider = init_slider(page1, 1,1,0,400, \"Blurryness threshold\", 50,1)\n",
    "blurryness_slider.config(command=change_blurryness)\n",
    "\n",
    "reflection_slider = init_slider(page1, 2,1,0, 1.0, \"Reflection threshold\", 0.05, 0.05)\n",
    "reflection_slider.config(command=change_reflection)\n",
    "yolo_confidence_score_slider = init_slider(page1, 3, 1, scale_from=0, scale_to=1, label_text=\"Yolo confidence threshold\", initial_value=0.8, resolution=0.05)\n",
    "yolo_confidence_score_slider.config(command=change_yolo_confidence)\n",
    "\n",
    "classification_confidence_score_slider = init_slider(page1, 4, 1, scale_from=0, scale_to=1, label_text=\"Classification threshold\", initial_value=0.9, resolution=0.05)\n",
    "classification_confidence_score_slider.config(command=change_classification_threshold)\n",
    "\n",
    "\n",
    "def on_capture_id_click(cur_sharp, sharp_hist, cur_refl, refl_hist,image):\n",
    "    global is_running\n",
    "    mean_refl = calculate_mean(refl_hist)\n",
    "    mean_sharp = calculate_mean(sharp_hist)\n",
    "    if cur_sharp < mean_sharp:\n",
    "        label_warning = tk.Label(page1, text=f\"image is blurry, please stand still\", font=(\"Arial\", 12))\n",
    "        label_warning.grid(row=1,column=1, padx=10, pady=5, sticky=\"w\") \n",
    "    elif cur_refl < mean_refl:\n",
    "        label_warning = tk.Label(page1, text=f\"Id card has reflections, please get rid of them\", font=(\"Arial\", 12))\n",
    "        label_warning.grid(row=1,column=1, padx=10, pady=5, sticky=\"w\") \n",
    "    else:\n",
    "        is_running = False\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()# Release the camera\n",
    "        show_page_2(image)\n",
    "        \n",
    "\n",
    "def calculate_mean(hist_list):\n",
    "    return int(sum(hist_list)/len(hist_list))\n",
    "               \n",
    "button_capture_id = tk.Button(page1, text=\"Capture ID\", font=(\"Arial\", 12), bg=\"orange\")\n",
    "button_capture_id.grid(row=2, column=0, padx=10, pady=10, sticky=\"w\")  # Align to the left\n",
    "\n",
    "\n",
    "label_info = init_text_label(page1,\"\", 1, 2)\n",
    "\n",
    "# label_text = tk.Label(window, text=\"text from passport\", font=(\"Arial\", 10), fg=\"#1c1c1e\")\n",
    "# label_text.grid(row=0, column=1, padx=10, pady=5) \n",
    "\n",
    "sharpness_hist = []\n",
    "reflection_score_hist = []\n",
    "start_time = time.time()\n",
    "def on_take_face_photo_click(img_1,img_2):\n",
    "    global is_running_face, similarity_label,model, yolo_model, live_face, face_image_from_id, video_label_face, button_analyse\n",
    "    model = None\n",
    "    yolo_model = None\n",
    "    \n",
    "    del model, yolo_model\n",
    "    is_running_face = False\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() # Release the camera\n",
    "    # Compare faces\n",
    "    live_face = img_1\n",
    "    face_image_from_id = img_2\n",
    "    show_image(live_face, video_label_face)\n",
    "    button_analyse.config(command = lambda: analyse_verify())\n",
    "    \n",
    "\n",
    "def analyse_verify():\n",
    "    global live_face, face_image_from_id, similarity_label,similarity_score\n",
    "    try:\n",
    "        result = DeepFace.verify(img1_path=face_image_from_id, img2_path=live_face, enforce_detection=False)\n",
    "        \n",
    "        similarity = 1-result[\"distance\"]\n",
    "        similarity_score = similarity\n",
    "        similarity_label.config(text=f\"Similarity score: {similarity:.2f}\")\n",
    "        \n",
    "        if result[\"verified\"]:\n",
    "            print(\"Match found!\")\n",
    "            similarity_label.config(text=f\"Similarity score: {similarity:.2f}, face verified!\")\n",
    "        else:\n",
    "            similarity_label.config(text=f\"Similarity score: {similarity:.2f}, face not verified!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "# Function to update frame in Tkinter window\n",
    "show_frame(page1)\n",
    "\n",
    "similarity_label = None\n",
    "def open_cap_face():\n",
    "    global is_running_face, cap, similarity_label, model\n",
    "    similarity_label = init_text_label(page2, \"similarity score: \", 2, 4)\n",
    "    cap = cv2.VideoCapture(cam)\n",
    "    update_frame_face()\n",
    "    \n",
    "new_height = int(300 * 1.5)\n",
    "new_width  = int(400 * 1.5)\n",
    "def update_frame_face():\n",
    "    global is_running_face,face_image_from_id, similarity_label, button_capture_face\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not is_running_face:\n",
    "        return  \n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        video_label_face.after(10, update_frame_face)  # Schedule next frame update\n",
    "        return\n",
    "    frame_video = cv2.resize(frame, (new_width,new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    live_face = find_face(frame)\n",
    "    \n",
    "    if live_face is not None:\n",
    "        \n",
    "       button_capture_face.config(command=lambda: on_take_face_photo_click(live_face, face_image_from_id))\n",
    "\n",
    "        # Draw rectangle around face\n",
    "        # cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    \n",
    "    show_image(frame_video, video_label_face)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    video_label_face.after(frame_delay, update_frame_face)\n",
    "\n",
    "def update_frame():\n",
    "    \n",
    "    global frame_counter, id_images_batch, id_labels,is_running\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not is_running:\n",
    "        return  \n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame. update_frame\")\n",
    "        window.after(10, update_frame)  # Schedule next frame update\n",
    "        return\n",
    "    \n",
    "    \n",
    "    frame_video = cv2.resize(frame, (new_width,new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Predict bounding box coordinates\n",
    "    bounding_box_prediction = yolo(yolo_model, frame_video)\n",
    "    \n",
    "    # Make siamese network work only if ID card is found\n",
    "    if bounding_box_prediction.xyxy[0].shape != (0, 6):\n",
    "        # Get coordinates of bounding box\n",
    "        \n",
    "        x1, y1, x2, y2, confidence_score_object_det = get_results_of_yolo(bounding_box_prediction)\n",
    "        \n",
    "        # Crop the ID card\n",
    "        cropped_id_card = crop_id_card(bounding_box_prediction, frame_video)\n",
    "        is_valid_for_anaysis, sharpness, refl_score = check_validity(cropped_id_card)\n",
    "        sharpness_hist.append(sharpness)\n",
    "        reflection_score_hist.append(refl_score)\n",
    "        if is_valid_for_anaysis:\n",
    "            # sharpened_cropped_id = sharpen_with_kernel(cropped_id_card)\n",
    "            if confidence_score_object_det > yolo_confidence_score_threshold:\n",
    "                # Draw bounding box on the frame\n",
    "                cv2.rectangle(frame_video, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                text = f'{confidence_score_object_det:.2f}'\n",
    "                text_position = (x1, y1 - 10)\n",
    "                cv2.putText(frame_video, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                button_capture_id.config(command=lambda: on_capture_id_click(sharpness,sharpness_hist,refl_score,reflection_score_hist,cropped_id_card))\n",
    "            \n",
    "        label_info.config(text=f'sharpness: {sharpness}(mean: {calculate_mean(sharpness_hist)}), \\n reflection score: {refl_score:.2%}')\n",
    "\n",
    "    show_image(frame_video, video_label)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    video_label.after(frame_delay, update_frame)\n",
    "        \n",
    "\n",
    "# Start updating frames\n",
    "update_frame()\n",
    "\n",
    "# Define a function to handle window close\n",
    "def on_closing():\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()# Release the camera\n",
    "    window.destroy()  # Close the Tkinter window\n",
    "    \n",
    "# Bind the window close event\n",
    "window.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33310b0-43ee-47cf-a921-c3a4ea0b3c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
