{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806c8944-33f5-4219-ab8c-c5582a59a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Input,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import RandomCrop, RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras import backend as K\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a40f8-1356-4197-9e74-3e335c6061a9",
   "metadata": {},
   "source": [
    "# Train MobileNetV3\n",
    "To train our neural network we need:\n",
    " - Load the data\n",
    " - Downscale data and prepare it for input\n",
    " - create some neccessary functions to show image, show scores and etc\n",
    " - Load MobileNetV3 pretrained model without top\n",
    " - Prepare Data augmentation layers\n",
    " - Prepare output layers\n",
    " - Set neccessary parameters\n",
    " - Train the model\n",
    " - evaluate the model performance\n",
    " - create pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4c41f-f8d7-4386-bf8e-7e4642c1a0ac",
   "metadata": {},
   "source": [
    "#### parameters initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590f9b31-668a-4661-b852-f0142b3e3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set seed for any randomizing that will happen during process\n",
    "SEED = 44\n",
    "# choose how much times we want data to be shown\n",
    "epochs = 5\n",
    "BATCH_SIZE = 8\n",
    "# validation frequency so validation wont be run every epoch\n",
    "VAL_FREQ = 2\n",
    "OPTIMIZER = 'adam'\n",
    "shape_mobilenet = 224\n",
    "VALIDATION_SPLIT = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ab60a-e755-4b52-b3bc-d1962afff857",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2cfb7c-0260-4b13-bb44-48940102624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01_alb_id', '02_aut_drvlic_new', '04_aut_id', '07_chl_id', '09_chn_id', '10_cze_id', '12_deu_drvlic_new', '13_deu_drvlic_old', '19_esp_drvlic', '22_est_id', '23_fin_drvlic', '24_fin_id', '26_hrv_drvlic', '29_irn_drvlic', '30_ita_drvlic', '31_jpn_drvlic', '33_mac_id', '35_nor_drvlic', '36_pol_drvlic', '37_prt_id', '38_rou_drvlic', '40_srb_id', '42_svk_id', '43_tur_id']\n"
     ]
    }
   ],
   "source": [
    "all_types = os.listdir(\"./jpgs/\")\n",
    "print(all_types)\n",
    "all_images_dict = {}\n",
    "all_full_names = {}\n",
    "for type in all_types:\n",
    "    all_images_dict[type[:2]] = []\n",
    "\n",
    "for type in all_types:\n",
    "    images_of_one_type = []\n",
    "    for condition in os.listdir(f\"./jpgs/{type}\"):\n",
    "        \n",
    "        condition_images = os.listdir(f\"./jpgs/{type}/{condition}\")\n",
    "        images_of_one_type += condition_images\n",
    "        for img in condition_images:\n",
    "            all_full_names[img] = f\"./jpgs/{type}/{condition}/{img}\"\n",
    "    \n",
    "    all_images_dict[type[:2]] = images_of_one_type\n",
    "        \n",
    "        \n",
    "        \n",
    "        # all_images_dict[type].append()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f91926-f4aa-437b-9aab-0d96f65e4380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7064"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2733ef48-467c-4b49-be77-5fd96309c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "labels = []\n",
    "for key in all_images_dict:\n",
    "    for image in all_images_dict[key]:\n",
    "        image_paths.append(image)\n",
    "        labels.append(image[:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be26838-37be-4185-b9e0-2a7db30b75af",
   "metadata": {},
   "source": [
    "## Create pairs for every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc8bcac-9d61-45f4-ac69-8575ef0fdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_pairs(image_paths, labels, num_positive_pairs=15, num_negative_pairs=35):\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "\n",
    "    id_numbers = [label[2:] for label in labels]\n",
    "    \n",
    "\n",
    "    unique_ids = np.unique(id_numbers)\n",
    "    \n",
    "    # Group images by their ID numbers\n",
    "    id_indices = {id_num: []  for id_num in unique_ids}\n",
    "    for key in id_indices:\n",
    "        for image in image_paths:\n",
    "            if key == image[2:4]:\n",
    "                id_indices[key].append(image)\n",
    "    # print(id_indices)\n",
    "    for idx1 in range(len(image_paths)):\n",
    "        image = image_paths[idx1]\n",
    "        id_1 = image[2:4]\n",
    "        \n",
    "        pos_indices = np.array(id_indices[id_1])\n",
    "        pos_indices = pos_indices[pos_indices != image]\n",
    "        # print(pos_images)\n",
    "        selected_pos_indices = np.random.choice(pos_indices, size=min(len(pos_indices), num_positive_pairs), replace=False)\n",
    "        # print(selected_pos_indices)\n",
    "        for selected_image in selected_pos_indices:\n",
    "            image_1 = all_full_names[image_paths[idx1]]\n",
    "            image_2 = all_full_names[selected_image]\n",
    "            pairs.append([image_1, image_2])\n",
    "            pair_labels.append(1)  # Positive pair\n",
    "        \n",
    "        \n",
    "        # print(pairs)\n",
    "        # print(pair_labels)\n",
    "\n",
    "        for _ in range(num_negative_pairs):\n",
    "            id2 = np.random.choice(unique_ids)\n",
    "            # print(id2)\n",
    "            while id2 == id_1:  # Ensure the negative pair is from a different ID\n",
    "                id2 = np.random.choice(unique_ids)\n",
    "            neg_image = np.random.choice(id_indices[id2])\n",
    "            # neg_image = random.choice(id_indices[idx2])\n",
    "            # print(neg_image)\n",
    "            image_1 = all_full_names[image_paths[idx1]]\n",
    "            image_2 = all_full_names[neg_image]\n",
    "            \n",
    "            pairs.append([image_1, image_2])\n",
    "            pair_labels.append(0)  # Negative pair\n",
    "        \n",
    "        \n",
    "    return np.array(pairs), np.array(pair_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47ff99-ed6c-4f8a-a85e-508de4ca3b1f",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006841c0-d9b8-4c7b-bbd2-fc356f0e51de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353200\n"
     ]
    }
   ],
   "source": [
    "X,Y = create_pairs(image_paths, labels)\n",
    "print(len(X))\n",
    "# print(X[219850],Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd034c8a-3c22-484f-a786-b4f4986e81ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffled successfully\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(SEED)\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "if len(X) == len(Y):\n",
    "    # Shuffle both X and Y using the same indices\n",
    "    image_pairs = X[shuffled_indices]\n",
    "    labels = Y[shuffled_indices]\n",
    "    print(\"shuffled successfully\")\n",
    "else:\n",
    "    print(\"X and Y are not the same length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a153d0-3407-411a-95ae-0f8e051d0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your image dimensions and batch size\n",
    "IMG_HEIGHT = 300\n",
    "IMG_WIDTH = 300\n",
    "\n",
    "# Preprocessing function for a single image\n",
    "def preprocess_image(image_path):\n",
    "    \n",
    "    # Load and decode image from file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3) \n",
    "\n",
    "    # Resize image\n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "    # Normalize pixel values to the range [0.0, 1.0]\n",
    "    image = image / 255.0\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# Preprocessing function for image pairs\n",
    "def preprocess_image_pair(image_pair, label):\n",
    "    image_path1, image_path2 = image_pair[0], image_pair[1]\n",
    "\n",
    "    # Preprocess both images in the pair\n",
    "    image1 = preprocess_image(image_path1)\n",
    "    image2 = preprocess_image(image_path2)\n",
    "\n",
    "    return (image1, image2), label\n",
    "    \n",
    "# Convert the list of image pairs and labels into a tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_pairs, labels))\n",
    "\n",
    "# Map the preprocessing function to the dataset\n",
    "dataset = dataset.map(lambda x, y: preprocess_image_pair(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Calculate the number of samples for the train/val split\n",
    "total_samples = len(image_pairs)\n",
    "train_size = int((1 - VALIDATION_SPLIT) * total_samples)\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_dataset = dataset.take(train_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = dataset.skip(train_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe778d-0385-473c-bd28-4c51178d1910",
   "metadata": {},
   "source": [
    "# Add Data augmentation to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bbae64-30d5-4b13-a503-9779033b8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(X):\n",
    "    X = RandomCrop(height=shape_mobilenet,width=shape_mobilenet,seed=SEED)(X)\n",
    "    X = RandomFlip(mode=\"horizontal_and_vertical\",seed=SEED)(X)\n",
    "    X = RandomRotation(factor=0.9)(X)\n",
    "    X = RandomContrast(0.8,seed=SEED)(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0836c8e-ec32-450a-a14e-81043795ec54",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d1e494-3b3b-413c-9d2e-03fd51d7e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV3Large(input_shape=(shape_mobilenet,shape_mobilenet,3),weights='imagenet',include_top=False,include_preprocessing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "606e6046-0165-4391-800a-d3a85060c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 1024\n",
    "# define model neurons for the last Dense layer and model name for saving it during training and later \n",
    "model_name = f'MobileNetV3_{num_neurons}_v2_siamese.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a99c487b-6424-4cea-a334-8a87e059e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape for each image in the pair (300x300 RGB image)\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "# Input layers for the two images so inputs can be passed through the same network branch later\n",
    "input_1 = layers.Input(shape=input_shape, name='input_image_1')\n",
    "input_2 = layers.Input(shape=input_shape, name='input_image_2')\n",
    "\n",
    "# Data augmentation for both inputs\n",
    "x1 = data_augment(input_1)\n",
    "x2 = data_augment(input_2)\n",
    "\n",
    "# Pass both inputs through the same base model\n",
    "x1 = base_model(x1)\n",
    "x2 = base_model(x2)\n",
    "\n",
    "# Global spatial avg pooling layer for both outputs\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "\n",
    "# Calculate the L1 distance between the two outputs\n",
    "x = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([x1, x2])\n",
    "\n",
    "# Fully connected layer\n",
    "x = Dense(num_neurons,activation='relu',kernel_regularizer=l2(0.02))(x) #\n",
    "x = Dropout(0.7)(x)\n",
    "\n",
    "\n",
    "output_len = 2  # two classes 0 = not the same , 1 = the same \n",
    "predictions = Dense(output_len, activation='softmax')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=[input_1, input_2], outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a718e5-4092-4b69-b2c1-56c4e4628453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized) for them to properly initialize\n",
    "# i.e. freeze all convolutional MobileNetV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08dd3f2d-0963-44ea-b93c-5b268170a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f6055ed-9691-4bdd-a680-b1326d33a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4cdf0-257a-425a-bda8-0e6e963862bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the new data for one epoch\n",
    "epochs = 1\n",
    "\n",
    "hist = model.fit(train_dataset,\n",
    "                 epochs = epochs,\n",
    "                 callbacks = [checkpoint,early_stopping]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e174f2e-f6ce-4a5e-a2e5-4bae94fcfccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from MobileNet V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "# for i, layer in enumerate(model.layers):\n",
    "#    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b6894ce-82cf-422b-8b8d-8e1acfc2092a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ./MobileNetV3_512_512_siamese.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model_name)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./MobileNetV3_512_512_siamese.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_name):\n\u001b[1;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\saving\\save.py:204\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    203\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(filepath_str, \u001b[38;5;28mcompile\u001b[39m, options)\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at ./MobileNetV3_512_512_siamese.h5"
     ]
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model\n",
    "\n",
    "model = load_model(\"./MobileNetV3_512_512_siamese.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afd52953-7119-44ff-b33b-efdc56e19888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b864c68-5b8e-4256-b103-41f2e04040ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fec7fe1f-8b52-46ba-a82d-a433337cd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use adam with a low learning rate\n",
    "BATCH_SIZE = 8\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e1277ec-64a1-40d8-a0ad-e86fba9ffd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "41943/41943 [==============================] - 6609s 157ms/step - loss: 0.6107 - accuracy: 0.7003 - val_loss: 0.6151 - val_accuracy: 0.6950\n",
      "Epoch 2/5\n",
      " 2887/41943 [=>............................] - ETA: 1:26:23 - loss: 0.6132 - accuracy: 0.6974"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we train our model again, this time fine-tuning the convolutional layers\n",
    "# alongside the top Dense layers\n",
    "epochs = 5\n",
    "hist_unfreezed = model.fit(train_dataset,\n",
    "                 epochs = epochs,\n",
    "                 validation_data=validation_dataset,\n",
    "                 callbacks = [checkpoint,early_stopping]\n",
    "                 # ,validation_freq=VAL_FREQ\n",
    "                )\n",
    "\n",
    "\n",
    "#  https://keras.io/api/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30db2c-1935-4d50-9577-e025ed26c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model\n",
    "\n",
    "# model = load_model('mobilenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6353be-79a7-46c6-87c7-3c86f63bb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf204ca-559d-41fe-91ca-4af97d693483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(img_1_path,img_2_path,model):\n",
    "    # img_path = 'dawg.jpg'\n",
    "    img_1 = np.array(preprocess_image(img_1_path))\n",
    "    # img_2 = np.array(preprocess_image(img_2_path))\n",
    "    # print(img_1.shape)\n",
    "    # print(img_2.shape)\n",
    "    img_1 = np.expand_dims(img_1, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    img_2 = np.expand_dims(img_2_path, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "\n",
    "    \n",
    "    \n",
    "    preds = model.predict((img_1,img_2))\n",
    "    print(preds)\n",
    "    \n",
    "    # decode the results into a list of tuples (class, description, probability)\n",
    "    # (one such list for each sample in the batch)\n",
    "    if preds[0][0] > 0.7:\n",
    "        predicted_idx = 0\n",
    "    elif preds[0][1] > 0.7:\n",
    "        predicted_idx = 1\n",
    "    else: \n",
    "        predicted_idx = 2\n",
    "    # predicted_idx = np.argmax(preds)\n",
    "    print(predicted_idx)\n",
    "    return predicted_idx, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc412c0a-7197-4032-8970-bd29101767ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'MobileNetV3_1024_siamese.h5'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7469a9-642b-45b9-9b0f-4064d9ceacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "myimg_path = \"./saba_1.jpg\"\n",
    "toko_img_path = \"./toko_1.jpg\"\n",
    "prava = \"./saba_prava.jpg\"\n",
    "predict(myimg_path,prava,model)\n",
    "img_1 = preprocess_image(myimg_path)\n",
    "# img_2 = preprocess_image(toko_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f00b02a-d993-489f-9dfb-025b87384eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "833ff2f2-4d22-45b5-8dab-a00e3b5bc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imghdr\n",
    "\n",
    "# Define the folder path and the valid image types\n",
    "folder_path = './jpgs/'\n",
    "valid_image_types = {'jpeg', 'png', 'gif', 'bmp'}\n",
    "\n",
    "def check_image_files(folder):\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_type = imghdr.what(file_path)\n",
    "            if file_type in valid_image_types:\n",
    "                # print(f\"{file} is a valid image type ({file_type.upper()})\")\n",
    "                pass\n",
    "            else:\n",
    "                os.remove(f\"{root}/{file}\")\n",
    "                print(f\"{root}/{file} is NOT a valid image type\")\n",
    "\n",
    "# Run the function\n",
    "check_image_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5e7dd-3fcc-4b9b-91bc-1add61243cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
