{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c722543-a20d-4fda-9931-9461f2e6892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Input,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import RandomCrop, RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras import backend as K\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "# Clear last session to feee up space before new\n",
    "K.clear_session()\n",
    "\n",
    "# if GPU is available this code will state 1\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# image height and width for input into Siamese Network with MobileNetV3 as a base \n",
    "IMG_HEIGHT = 300\n",
    "IMG_WIDTH = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87744d82-8f01-420c-82a8-0513700c7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Siamese network model file\n",
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model\n",
    "\n",
    "# Preprocessing function to resize and normalize image\n",
    "def preprocess_image(image):\n",
    "    # resize image to match height and width specified earlier \n",
    "    image_resized = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    \n",
    "    # Normalize values of pixels to be around 0.0-1.0\n",
    "    image_resized = image_resized / 255.0  \n",
    "    \n",
    "    return image_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51bd57ed-f88e-448b-bfce-8e14b644266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese network prediction function  \n",
    "def predict(id_images, img_to_evaluate, model, threshold = 0.65):\n",
    "    '''\n",
    "    id_images_batch - batch of saved id images, should be numpy array of shape (x, 300, 300, 3) where x is number of saved images \n",
    "    or path for image which is saved in the memory and should be compared to, in this case it is string path\n",
    "    img_to_evaluate - image which will be compared to img_base which is saved in the memory\n",
    "    model - AI model file which contains trained weights for siamese network\n",
    "    threshold - optional value which we can state for siamese networks confidence score from 0.50 till 0.99, which means \n",
    "    that results will be shown only if confidence score for similarity is more than threshold\n",
    "    '''\n",
    "    \n",
    "    # preprocess images\n",
    "    if type(id_images) == str:\n",
    "        img_base = np.array(preprocess_image(id_images))\n",
    "        img_base = np.expand_dims(img_base, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    elif isinstance(id_images, np.ndarray):\n",
    "        img_base = id_images\n",
    "\n",
    "    \n",
    "    img_to_evaluate = np.array(preprocess_image(img_to_evaluate))\n",
    "\n",
    "    # expand dimensions of images to add batch dimension \n",
    "    \n",
    "    img_to_evaluate = np.expand_dims(img_to_evaluate, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    \n",
    "    img_to_evaluate = np.tile(img_to_evaluate, (len(img_base), 1, 1, 1))\n",
    "    \n",
    "    # predict the similarity between the two images,\n",
    "    # returns tensor with two values like this [0.37, 0.63] which states how similar are images\n",
    "    # 0.0 no similarity , 1.0 full similarity\n",
    "    preds = model.predict((img_base,img_to_evaluate))\n",
    "    predicted_idx = np.argmax(preds[:, 1])\n",
    "\n",
    "    if preds[predicted_idx][1] > threshold:\n",
    "        return predicted_idx, preds[predicted_idx][1]\n",
    "    else:\n",
    "        return 999,999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aaa0955-860a-436d-921f-eb6d80b6f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Saba/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-11-23 Python-3.9.0 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# load the siamese network model\n",
    "model_name = f'MobileNetV3_1024_siamese.h5'\n",
    "model = load_model(model_name)\n",
    "# load the YOLO model \n",
    "yolo_model = torch.hub.load('ultralytics/yolov5',\"custom\", path=r'C:\\Users\\Saba\\Desktop\\Python\\vigo\\AIMV\\yolov5\\runs\\train\\exp6\\weights/best.pt',force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96227c48-ff49-424c-bbcb-0c2bf87ad05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cddec8c6-3b4a-4944-9808-293ab073cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns YOLO model object detection results \n",
    "def yolo(yolo_model,frame):\n",
    "    results = yolo_model(frame)\n",
    "    return results\n",
    "\n",
    "# unpack the coordinates of bounding boxes and confidence score for ID class\n",
    "def get_results_of_yolo(frame_predictions):\n",
    "    x1 = int(frame_predictions.xyxy[0][0][0])\n",
    "    y1 = int(frame_predictions.xyxy[0][0][1])\n",
    "    x2 = int(frame_predictions.xyxy[0][0][2])\n",
    "    y2 = int(frame_predictions.xyxy[0][0][3])\n",
    "    \n",
    "    confidence = frame_predictions.xyxy[0][0][4]\n",
    "    \n",
    "    return x1,y1,x2,y2,confidence\n",
    "\n",
    "# crop the id card from the frame for it to be saved or passed to siamese network for evaluating\n",
    "def crop_id_card(frame_predictions, frame):\n",
    "    x1,y1,x2,y2,confidence = get_results_of_yolo(frame_predictions)\n",
    "    # x1 (pixels)  y1 (pixels)  x2 (pixels)  y2 (pixels)   confidence   class\n",
    "    cropped_image = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cbf638-26d9-42fc-a30a-7cc7f6bfb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path for id_examples\n",
    "id_examples_filepath = './id_examples/'\n",
    "def retrieve_saved_ids():\n",
    "    saved_ids_labels = []\n",
    "    saved_ids_images = []\n",
    "    for id_img in os.listdir(id_examples_filepath):\n",
    "        id_image_uncropped = cv2.imread(id_examples_filepath+id_img)\n",
    "        bounding_box_prediction = yolo(yolo_model,id_image_uncropped)\n",
    "        \n",
    "        # make siamese network work only in the case of finding id card shown\n",
    "        if bounding_box_prediction.xyxy[0].shape != (0,6):\n",
    "            # get coordinates of bounding box\n",
    "            x1,y1,x2,y2,confidence_score = get_results_of_yolo(bounding_box_prediction)\n",
    "            id_image_cropped = crop_id_card(bounding_box_prediction, id_image_uncropped)\n",
    "            saved_ids_images.append(id_image_cropped)\n",
    "        else: \n",
    "            saved_ids_images.append(id_image_uncropped)\n",
    "            print(\"uncropped version added\")\n",
    "        saved_ids_labels.append(id_img)\n",
    "\n",
    "    id_img_batch = np.array([preprocess_image(image_array) for image_array in saved_ids_images])\n",
    "    return id_img_batch, saved_ids_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55ef5d2-dbdc-4926-a0fb-756e2916aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_id_to_database(frame, new_id_name):\n",
    "    # Create a directory to store new ID images if it doesnâ€™t exist\n",
    "    os.makedirs(\"./id_examples/\", exist_ok=True)\n",
    "\n",
    "    # Create a unique file name with ID type and current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path = f\"./id_examples/{new_id_name}_{timestamp}.jpg\"\n",
    "\n",
    "    # Save the image to the specified path\n",
    "    cv2.imwrite(file_path, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937263b6-a1ba-4f4b-b037-84152165132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_still_frame(current_frame, previous_frame, threshold=50000):\n",
    "#     # Calculate the absolute difference between frames\n",
    "#     diff = cv2.absdiff(previous_frame, current_frame)\n",
    "#     # Convert to grayscale and compute the sum of differences\n",
    "#     gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "#     non_zero_count = np.sum(gray_diff)\n",
    "#     return non_zero_count < threshold\n",
    "\n",
    "# def is_sharp_frame(frame, threshold=50):\n",
    "#     # Calculate the Laplacian to assess sharpness (higher variance = sharper)\n",
    "#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     variance = cv2.Laplacian(gray_frame, cv2.CV_64F).var()\n",
    "#     return variance > threshold\n",
    "\n",
    "def ask_user_for_id_type():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    new_id_type = simpledialog.askstring(\"Input\", \"Enter new ID type name:\")\n",
    "    root.destroy()\n",
    "    return new_id_type\n",
    "\n",
    "def check_for_new_card(confidence_score,frame):\n",
    "    if confidence_score > 0.90:\n",
    "        if frame_counter < 20:\n",
    "            frame_counter += 1\n",
    "            \n",
    "        else:\n",
    "            new_id_name = ask_user_for_id_type()\n",
    "            add_new_id_to_database(frame, new_id_name)\n",
    "            id_images_batch, id_labels = retrieve_saved_ids()\n",
    "            frame_counter = 0\n",
    "        return \"found_id\"\n",
    "    return \"no_id\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18298de4-c924-469a-967d-b0ad065763fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3959849619.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    global frame_counter = 0\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Open video capture (0 is the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "id_images_batch, id_labels = retrieve_saved_ids()\n",
    "global frame_counter = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Predict bounding box coordinates\n",
    "    bounding_box_prediction = yolo(yolo_model,frame)\n",
    "\n",
    "    # make siamese network work only in the case of finding id card shown\n",
    "    if bounding_box_prediction.xyxy[0].shape != (0,6):\n",
    "        # get coordinates of bounding box\n",
    "        x1,y1,x2,y2,confidence_score = get_results_of_yolo(bounding_box_prediction)\n",
    "        # Show the frame with prediction\n",
    "\n",
    "        # Crop the ID card\n",
    "        cropped_id_card = crop_id_card(bounding_box_prediction, frame)\n",
    "        \n",
    "        # Check if a new ID card is detected\n",
    "        if len(id_images_batch) == 0:\n",
    "            if confidence_score > 0.90:\n",
    "                if frame_counter < 20:\n",
    "                    frame_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    new_id_name = ask_user_for_id_type()\n",
    "                    add_new_id_to_database(frame, new_id_name)\n",
    "                    id_images_batch, id_labels = retrieve_saved_ids()\n",
    "                    frame_counter = 0\n",
    "                    \n",
    "        else:\n",
    "            # predict with siamese network\n",
    "            prediction_idx, confidence_score_siamese = predict(id_images_batch, cropped_id_card, model, threshold=0.95)\n",
    "\n",
    "            if prediction_idx == 999:\n",
    "                prediction_name = \"not found\"\n",
    "                if confidence_score > 0.90:\n",
    "                    if frame_counter < 20:\n",
    "                        frame_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        new_id_name = ask_user_for_id_type()\n",
    "                        add_new_id_to_database(frame, new_id_name)\n",
    "                        id_images_batch, id_labels = retrieve_saved_ids()\n",
    "                        frame_counter = 0\n",
    "            else:\n",
    "                prediction_name = id_labels[prediction_idx]\n",
    "\n",
    "            cv2.putText(frame, f'id type: {prediction_name},{confidence_score_siamese:.2f}', (10, 30),\n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        if confidence_score > 0.45:\n",
    "            # apply bounding box to the image\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # apply confidence score for the id object detection\n",
    "            text = f'{confidence_score:.2f}'\n",
    "            text_position = (x1, y1 - 10)  # Slightly above the bounding box\n",
    "            \n",
    "            # Put text on the frame\n",
    "            cv2.putText(frame, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f2da9b-23cf-417f-a1c3-68a6e709c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a button that prints the input field's text when clicked\n",
    "def on_button_click(id_image):\n",
    "    new_id_name = input_entry.get()  # Get text from entry field\n",
    "    add_new_id_to_database(id_image, new_id_name)\n",
    "    id_images_batch, id_labels = retrieve_saved_ids()\n",
    "    input_entry.delete(0, tk.END)  # Clear the entry field after clicking\n",
    "\n",
    "\n",
    "def create_window_submit_window():\n",
    "    window_submit_photo = tk.TK()\n",
    "    window_submit_photo.title(\"ID submission\")\n",
    "    window.geometry(\"1200x800\")\n",
    "    \n",
    "    video_label_submit = tk.Label(window_submit_photo)\n",
    "    video_label_submit.pack()\n",
    "    \n",
    "    input_entry = tk.Entry(window, width=30)\n",
    "    input_entry.pack()\n",
    "\n",
    "    button = tk.Button(window, text=\"Submit\", command=on_button_click)\n",
    "    button.pack()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49baddc0-7c14-46ae-9f3d-24e5b87af77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Assuming you have defined these functions: retrieve_saved_ids, yolo, get_results_of_yolo,\n",
    "# crop_id_card, ask_user_for_id_type, add_new_id_to_database, and predict.\n",
    "\n",
    "# Open video capture (0 is the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "id_images_batch, id_labels = retrieve_saved_ids()\n",
    "frame_counter = 0  # Use local variable instead of global\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"ID Detection\")\n",
    "window.geometry(\"1200x800\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a label to display the video feed\n",
    "video_label = tk.Label(window)\n",
    "video_label.pack()\n",
    "\n",
    "# Create an entry field\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to update frame in Tkinter window\n",
    "def update_frame():\n",
    "    global frame_counter, id_images_batch, id_labels\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        window.after(10, update_frame)  # Schedule next frame update\n",
    "        return\n",
    "\n",
    "    # Predict bounding box coordinates\n",
    "    bounding_box_prediction = yolo(yolo_model, frame)\n",
    "\n",
    "    # Make siamese network work only if ID card is found\n",
    "    if bounding_box_prediction.xyxy[0].shape != (0, 6):\n",
    "        # Get coordinates of bounding box\n",
    "        x1, y1, x2, y2, confidence_score = get_results_of_yolo(bounding_box_prediction)\n",
    "\n",
    "        # Crop the ID card\n",
    "        cropped_id_card = crop_id_card(bounding_box_prediction, frame)\n",
    "\n",
    "        # Check if a new ID card is detected\n",
    "        if len(id_images_batch) == 0:\n",
    "            if confidence_score > 0.90:\n",
    "                if frame_counter < 20:\n",
    "                    frame_counter += 1\n",
    "                else:\n",
    "                    new_id_name = ask_user_for_id_type()\n",
    "                    add_new_id_to_database(frame, new_id_name)\n",
    "                    id_images_batch, id_labels = retrieve_saved_ids()\n",
    "                    frame_counter = 0\n",
    "        else:\n",
    "            # Predict with siamese network\n",
    "            prediction_idx, confidence_score_siamese = predict(id_images_batch, cropped_id_card, model, threshold=0.95)\n",
    "\n",
    "            if prediction_idx == 999:\n",
    "                prediction_name = \"not found\"\n",
    "                if confidence_score > 0.90:\n",
    "                    if frame_counter < 20:\n",
    "                        frame_counter += 1\n",
    "                    else:\n",
    "                        new_id_name = ask_user_for_id_type()\n",
    "                        add_new_id_to_database(frame, new_id_name)\n",
    "                        id_images_batch, id_labels = retrieve_saved_ids()\n",
    "                        frame_counter = 0\n",
    "            else:\n",
    "                prediction_name = id_labels[prediction_idx]\n",
    "\n",
    "            cv2.putText(frame, f'id type: {prediction_name}, {confidence_score_siamese:.2f}', (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        if confidence_score > 0.45:\n",
    "            # Draw bounding box on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            text = f'{confidence_score:.2f}'\n",
    "            text_position = (x1, y1 - 10)\n",
    "            cv2.putText(frame, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Convert frame to RGB and update the Tkinter label\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_label.imgtk = imgtk\n",
    "    video_label.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    video_label.after(10, update_frame)\n",
    "\n",
    "# Start updating frames\n",
    "update_frame()\n",
    "\n",
    "# Define a function to handle window close\n",
    "def on_closing():\n",
    "    cap.release()  # Release the camera\n",
    "    window.destroy()  # Close the Tkinter window\n",
    "\n",
    "# Bind the window close event\n",
    "window.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca797e-02a1-40c7-a9fa-3703c0290346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
